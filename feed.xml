<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://lathashree01.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://lathashree01.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-05T15:28:45+00:00</updated><id>https://lathashree01.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">ML Model Card: What should you include?</title><link href="https://lathashree01.github.io/blog/2025/ml-model-card-what-should-you-include/" rel="alternate" type="text/html" title="ML Model Card: What should you include?"/><published>2025-07-02T18:06:51+00:00</published><updated>2025-07-02T18:06:51+00:00</updated><id>https://lathashree01.github.io/blog/2025/ml-model-card-what-should-you-include</id><content type="html" xml:base="https://lathashree01.github.io/blog/2025/ml-model-card-what-should-you-include/"><![CDATA[<p>ML model card captures thorough information such as datasets used for development, intended usage, failure cases and evaluation results and much more.</p> <p>Continuing from my previous article on <a href="https://medium.com/@lathashreeh/ai-governance-how-should-we-do-it-1d24b928135c">Responsible AI</a>, where I summarised Google’s responsible AI progress report. I wanted to document further about the concept of “Model card” — its general structure, purpose and why it&#39;s needed in ML workflows.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1006/1*EFUqWzVWCc52WCk3_LycnA.png"/><figcaption>Google’s recommendation of a model card [from a <a href="https://arxiv.org/abs/1810.03993">paper</a> published in 2019]</figcaption></figure> <p>A model card is a standardised documentation framework introduced to enhance transparency and accountability in ML systems. It provides critical metadata about an ML model. Typically, a well-structured model card could include sections such as:</p> <ul><li><strong>Model details</strong>: A description of what the model does, its architecture, and its version.</li><li><strong>Intended use</strong>: Context and applications where the model is suitable, along with explicitly stated <em>out-of-scope</em> use cases.</li><li><strong>Training and evaluation datasets</strong>: Detailed information on the datasets used for training, validation, and testing, including any preprocessing steps.</li><li><strong>Factors: </strong>Model cards ideally provide a summary of model performance across a variety of relevant factors, including groups, instrumentation, and environments.</li><li><strong>Metrics</strong>: The chosen evaluation metrics should reflect the potential real-world impacts of the model.</li><li><strong>Ethical considerations</strong>: Acknowledgement of potential harms, misuse scenarios or societal impacts.</li><li><strong>Limitations and recommendations</strong>: Areas where the model may underperform or produce unreliable results. Any recommendations towards using the model could also be provided to give a better understanding to the user.</li></ul> <p>Model cards act as an internal checkpoint during development and a communication tool for external stakeholders. They also support reproducibility, regulatory compliance, and responsible deployment practices, especially important as ML systems continue to be integrated into high-stakes domains. There are also multiple variations of model cards, such as those used for <a href="https://github.com/openai/gpt-3/blob/master/model-card.md">GPT-3</a> or <a href="https://huggingface.co/docs/hub/en/model-cards">models hosted on Hugging Face</a>.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/714/1*EbEzJqATq7fznLbNR_iqtg.png"/><figcaption>An example model card from the paper</figcaption></figure> <p>Essentially, I strongly believe the model cards are not one-size-fits-all. The depth and level of details often depend on who is using the model and how mature the team’s MLOps practices are. Nonetheless, model cards should be treated as living documents, something you revisit, refine and improve over time as part of building AI responsibly and transparently.</p> <h3>References</h3> <ul><li>Paper “Model cards for Model Reporting”: <a href="https://arxiv.org/pdf/1810.03993">https://arxiv.org/pdf/1810.03993</a></li><li>HuggingFace write up on Model Cards: <a href="https://huggingface.co/docs/hub/en/model-cards">https://huggingface.co/docs/hub/en/model-cards</a></li><li>OpenAI GPT-3 Model Card: <a href="https://github.com/openai/gpt-3/blob/master/model-card.md">https://github.com/openai/gpt-3/blob/master/model-card.md</a></li></ul> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=7a5f45cc9d36" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">AI governance — How should we do it?</title><link href="https://lathashree01.github.io/blog/2025/ai-governancehow-should-we-do-it/" rel="alternate" type="text/html" title="AI governance — How should we do it?"/><published>2025-06-27T00:13:10+00:00</published><updated>2025-06-27T00:13:10+00:00</updated><id>https://lathashree01.github.io/blog/2025/ai-governancehow-should-we-do-it</id><content type="html" xml:base="https://lathashree01.github.io/blog/2025/ai-governancehow-should-we-do-it/"><![CDATA[<h3>AI governance -How should we do it?</h3> <p>Building AI responsibly requires collaboration among researchers, developers, regulators, and end users. Embedding ethical safeguards and governance practices throughout the AI lifecycle helps foster trust in AI systems and ensures alignment with human values.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*RZGG5kydj81yo9NS"/><figcaption>Photo by <a href="https://unsplash.com/@amstram?utm_source=medium&amp;utm_medium=referral">Scott Graham</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure> <h3>What is AI Governance?</h3> <p>AI governance refers to the framework of policies, processes and practices that ensure the responsible development and use of AI systems. It aims to maximise the benefits of AI while minimising potential harms and risks, such as bias, lack of transparency and ethical concerns. Effective AI governance requires collaboration between various stakeholders, including governments, tech companies, researchers, and the public.</p> <h3>AI Governance in Practice: Google’s Approach</h3> <p>In February 2025, Google published a detailed report outlining its end-to-end AI governance framework, showcasing how it builds responsible AI at scale. The report explains how Google integrates responsible practices across infrastructure, model development, and product deployment to manage AI systems at scale.</p> <h3>What is “Full-Stack” Governance?</h3> <p>Google’s framework emphasises that governance cannot be an afterthought but must be embedded throughout the AI lifecycle, involving:</p> <ul><li>Data curation and documentation</li><li>Model development and evaluation</li><li>Application design and launch</li><li>Post-deployment monitoring and iteration</li></ul> <p>This “full-stack” perspective combines technical, organisational, and procedural elements to embed responsibility into every stage.</p> <h3>Key highlights</h3> <p>Google’s governance principles and approach are based on the <a href="https://airc.nist.gov/airmf-resources/playbook/">NIST AI Risk Management Framework</a> and focus on governance, mapping, measuring, and managing risks.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/706/1*rd87MK8vXutAP1QSk-ke-g.png"/><figcaption>Google’s AI governance approach</figcaption></figure> <h3>Tackling pre- and post-launch reviews of AI systems</h3> <h4>Model requirements</h4> <p>Google mandates that all models meet rigorous standards for data quality, performance, and compliance:</p> <ul><li>Training data is filtered to remove harmful or low-quality content.</li><li>Models must adhere to defined responsible AI principles.</li><li>Each model is accompanied by technical reports and model cards, which document performance, intended use, and limitations.</li></ul> <h4>Application requirements</h4> <p>Before any AI-powered application goes live, it must pass several governance gates:</p> <ul><li>Risk assessments, testing and design guidance</li><li>Human-in-the-loop design reviews</li><li>Adherence to launch protocols tailored for high-risk domains (e.g., health, applications developed for minors)</li></ul> <h4>Leadership reviews</h4> <p>One of the report’s standout features is the emphasis on cross-functional leadership reviews. Executives with expertise in responsible AI are directly involved in go/no-go decisions for launches, reinforcing accountability at the highest levels.</p> <h4>Post-launch requirements</h4> <p>Governance continues <strong>after launch</strong>, with ongoing evaluations of:</p> <ul><li>Emerging risks (such as model drifts, misuse of products)</li><li>User feedback and real-world performance</li><li>Gaps in mitigation strategies and policies that only appear at scale</li></ul> <p>Post-launch reviews are treated not as optional retrospectives, but as core governance events.</p> <h4>Infrastructure used for launching applications</h4> <p>The infrastructure is continuously streamlined for AI applications, responsibility testing and progress monitoring.</p> <h3>Documentation</h3> <p>Google places significant emphasis on rigorous model documentation as part of its AI governance framework. Technical reports are routinely published for advanced models, offering in-depth detail on model design, training data, evaluation procedures, and intended use cases. These reports are complemented by model cards, which present key information in a standardised, accessible format designed for developers and for policy stakeholders.</p> <h4>Model Cards</h4> <p>External model cards and technical reports are published regularly as transparency artefacts. Below is an example of Google’s suggestion for a model card in 2019 [<a href="https://arxiv.org/pdf/1901.10002">paper</a>]. While the format has evolved, the core principle of transparent, standardised model documentation gives a really good picture of the model’s development and capabilities.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/677/1*vksnw0ns5mprA9yj3EGZxg.png"/><figcaption>Google’s model card suggestion from 2019</figcaption></figure> <h4>Data and Model lineage</h4> <p>To support responsible scaling of AI systems, Google is also investing in infrastructure for data and model lineage tracking, which involves maintaining end-to-end visibility into the lifecycle of datasets and models. Such lineage systems are critical for debugging, compliance, and auditability, particularly in high-stakes applications where traceability is non-negotiable. By embedding lineage as a core technical capability, Google ensures that every AI system it deploys can be examined retrospectively with clarity on provenance and transformation history.</p> <h3>Conclusion</h3> <p>Overall, Google’s report presents a thorough and technically grounded AI governance framework. Its focus on documentation, post-deployment oversight, and lineage demonstrates an institutional commitment to responsible AI.</p> <p>While the specific mechanisms may vary across organisations, the underlying principles of transparency, traceability and cross-functional oversight are essential components of responsible AI development.</p> <h3>References</h3> <ul><li>Google’s responsible AI progress report: <a href="https://ai.google/static/documents/ai-responsibility-update-published-february-2025.pdf">https://ai.google/static/documents/ai-responsibility-update-published-february-2025.pdf</a></li><li>NIST AI Risk Management Framework: <a href="https://airc.nist.gov/airmf-resources/playbook/">https://airc.nist.gov/airmf-resources/playbook/</a></li><li>“Model Cards for Model Reporting” paper: <a href="https://arxiv.org/abs/1810.03993">https://arxiv.org/abs/1810.03993</a></li></ul> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=1d24b928135c" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">RAG architecture — keeping it simple</title><link href="https://lathashree01.github.io/blog/2025/rag-architecturekeeping-it-simple/" rel="alternate" type="text/html" title="RAG architecture — keeping it simple"/><published>2025-06-09T13:05:33+00:00</published><updated>2025-06-09T13:05:33+00:00</updated><id>https://lathashree01.github.io/blog/2025/rag-architecturekeeping-it-simple</id><content type="html" xml:base="https://lathashree01.github.io/blog/2025/rag-architecturekeeping-it-simple/"><![CDATA[<h3>Simple RAG architecture design -Indexing and Inference pipelines</h3> <p>Grounding the LLM to the data you care about.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*L4GsVEZuOEireODg"/><figcaption>Photo by <a href="https://unsplash.com/@kellysikkema?utm_source=medium&amp;utm_medium=referral">Kelly Sikkema</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure> <p>As our interactions with AI and conversational systems become increasingly common, we’ve all experienced moments where a chatbot drifts off-topic, discussing irrelevant information when we’re seeking answers grounded in a specific context. The key challenge is ensuring that the chatbot stays focused on the data or document we care about and generates responses based on that context. This is precisely where <strong>Retrieval-Augmented Generation (RAG)</strong> proves valuable.</p> <h3><strong>Retrieval-Augmented Generation</strong></h3> <p>RAG is a hybrid architecture that combines two key components: a <strong>retriever</strong> and a <strong>generator</strong>.</p> <p>The retriever is responsible for fetching relevant pieces of information from a large corpus or knowledge base, while the generator (typically a large language model) uses this retrieved content to craft a context-aware response. This architecture aims to ground the model’s output in facts and domain-specific data, thus improving relevance, factual accuracy, and controllability.</p> <p>Traditional LLMs, when used in isolation, rely solely on their pre-trained parameters to answer questions. This leads to a few key limitations:</p> <ul><li>They may hallucinate facts.</li><li>They cannot access new or proprietary information unless they are fine-tuned on it.</li><li>They struggle to stay within a defined domain of knowledge.</li></ul> <p>RAG addresses these issues by introducing an <strong>external source of truth</strong> during the inference process.</p> <p>A typical RAG system consists of two main stages: <strong>indexing</strong> and <strong>inference</strong>.</p> <h4>Indexing Phase</h4> <p>This phase involves preparing and storing the documents in a form suitable for retrieval at inference time. A simple indexing pipeline may look like the following:</p> <ol><li><strong>Document Ingestion</strong><br/> Start with a collection of documents, which may be in formats such as PDFs, Word files, HTML, or plain text.</li><li><strong>Parsing and Preprocessing</strong><br/>Use document parsers (e.g. PDF readers) to extract raw text content. This may also include removing boilerplate, normalising whitespace, or handling non-textual elements.</li><li><strong>Chunking</strong><br/> Split the documents into smaller, manageable chunks (by paragraphs or sliding windows of N tokens). It’s important that each chunk contains enough and semantically meaningful information, so it can provide relevant context to our LLM during generation.</li><li><strong>Embedding Generation</strong><br/>Use a sentence embedding model (e.g. Sentence-BERT, OpenAI’s text-embedding-3-small, or Cohere embeddings) to convert each chunk into a dense vector representation.</li><li><strong>Storage in Vector Database</strong><br/>Store these embeddings in a vector database alongside metadata like document ID, source and text. This allows for efficient similarity search for retrieval at inference time.</li></ol> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*5p6BBWcTz1qlMLp4WqoW4g.png"/><figcaption>Simple document indexing pipeline for RAG systems</figcaption></figure> <h4>Inference Phase</h4> <p>When a user submits a query, the system goes through the following steps:</p> <ol><li><strong>Query Embedding</strong><br/>The input query is converted into a dense vector using the same embedding model employed during the indexing phase. This ensures alignment in vector space between the query and document chunks.</li><li><strong>Context Retrieval</strong><br/>The query embedding is used to perform a similarity search, typically using cosine similarity or approximate nearest neighbours, in the vector database. This retrieves the top-K most relevant document chunks based on semantic similarity.</li><li><strong>LLM Input Construction</strong><br/>The retrieved chunks are formatted and injected into the input prompt of the language model, along with the original user query.</li><li><strong>LLM Generation and Output Post-processing</strong><br/>The LLM generates a response grounded in the retrieved context. This output may be post-processed to enforce constraints such as user-specific policies, safety filters, or formatting requirements. The final response is then returned to the user.</li></ol> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*iGUAbosUUqu9lkX2pYznlA.png"/><figcaption>Simple RAG-based inference pipeline</figcaption></figure> <h3>Conclusion</h3> <p>RAG provides a robust framework for building context-aware, domain-specific AI systems by combining the strengths of information retrieval with large language models. By grounding responses in retrieved content, RAG systems improve factual accuracy and enable dynamic knowledge integration.</p> <p>There are numerous optimisation strategies for deploying RAG-based applications in production. I highly encourage exploring these in depth, and I plan to share some of the techniques I have used in my own projects in upcoming articles.</p> <p>Ultimately, as the demand for trustworthy and adaptable AI grows, RAG stands out as a practical and scalable solution for real-world applications.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=17677ee3ade9" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Understanding the LLM’s inference</title><link href="https://lathashree01.github.io/blog/2025/understanding-the-llms-inference/" rel="alternate" type="text/html" title="Understanding the LLM’s inference"/><published>2025-06-08T10:03:13+00:00</published><updated>2025-06-08T10:03:13+00:00</updated><id>https://lathashree01.github.io/blog/2025/understanding-the-llms-inference</id><content type="html" xml:base="https://lathashree01.github.io/blog/2025/understanding-the-llms-inference/"><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*80b4bJAk8gU8EC4Z"/><figcaption>Photo by <a href="https://unsplash.com/@dbeamer_jpg?utm_source=medium&amp;utm_medium=referral">Drew Beamer</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure> <p>LLMs have revolutionised natural language understanding and generation. Whether you are building chatbots, drafting emails, or generating code, a trained model runs in the backend to produce outputs from users’ inputs. However, the process of inference is more nuanced than “feed in a prompt and receive text.”</p> <p>In this article, we will explore the two main stages of autoregressive LLM inference, namely as <strong>Prefill</strong> phase and the <strong>Decode</strong> phase. I will also delve into practical techniques for controlling output length, mitigating repetitiveness, and leveraging beam search.</p> <p>By the end, you should have a better technical understanding of how LLMs generate text and how you can steer them towards desired behaviours.</p> <h3>Overview of Autoregressive Inference</h3> <p>Most recent LLMs (e.g. GPT-style models) are autoregressive, meaning they generate text one token at a time, conditioning on all previously generated tokens.</p> <p>At a high level, the process has two phases:</p> <ol><li><strong>Prefill (Prompt Encoding):</strong> The model processes the user’s prompt in full, producing hidden states (and typically key-value caches) that summarise the prompt’s content.</li><li><strong>Decode (Autoregressive Generation):</strong> The model enters a loop, where at each step it takes the hidden state of the tokens generated so far, predicts a distribution over the next token, samples or selects one, appends it to the sequence, and updates its hidden state (via cached keys/values).</li></ol> <p>Understanding the distinction between these phases is crucial for optimising latency and for implementing decoding strategies that influence output quality.</p> <h3>Prefill Phase</h3> <p>When you supply a prompt, say, “Once upon a time, in a distant kingdom…” the model must convert each token in that input text into internal representations. This involves several steps:</p> <h4>Tokenisation</h4> <p>The input text is split into units called <em>tokens</em>, which are then encoded to obtain corresponding numbers (called <em>token_id) </em>for the substrings from a predefined vocabulary.</p> <h4>Embedding Lookup</h4> <p>Each <em>token_id</em> is mapped to a dense vector (<em>token embedding</em>), and a <em>positional embedding</em> is added to capture the positional information of each token in the sequence.</p> <h4>Attention Mechanism</h4> <p>The embedded tokens are processed by a stack of transformer blocks, each comprising:</p> <ul><li><strong>Self-attention</strong> enables each token to attend to all others in the input sequence, capturing contextual relationships.</li><li><strong>Feed-forward networks</strong> apply nonlinear transformations to each token.</li></ul> <h4>Key-Value Cache Construction</h4> <p>In decoder-only architectures, each transformer layer outputs <em>key</em> and <em>value</em> matrices used in future attention computations. Caching these avoids recomputing attention from scratch at every decode step.</p> <p>Once the prefill is done, the model holds a “snapshot” of hidden states and caches representing the entire input sequence.</p> <h3>Practical Considerations</h3> <h4>Latency vs. Throughput</h4> <ul><li>Prefill is a one-time cost per prompt.</li><li>Decode occurs once per generated token.</li><li>For long completions, caching substantially reduces compute; for short completions, the prefill overhead might dominate.</li></ul> <h4>Batching</h4> <ul><li>Multiple prompts can be processed in parallel during prefill.</li><li>However, batching increases the chance that fast requests are delayed by slow ones.</li><li>Optimal batch sizing balances GPU utilisation and end-to-end latency.</li></ul> <h4>Memory Footprint</h4> <ul><li>Caches grow with sequence length, hidden size, and number of layers.</li><li>For high-throughput systems, careful memory management is essential, especially with long prompts or large batch sizes.</li></ul> <h3>Decode Phase</h3> <p>After prefill, the model enters the autoregressive loop:</p> <ol><li>Take the hidden state of the last token.</li><li>Compute attention over cached key-value pairs.</li><li>Predict the distribution over the vocabulary.</li><li>Determine the next token (via greedy, top-k, nucleus sampling, or beam search).</li><li>Append the token to the sequence.</li><li>Update the cache with the new token’s key and value.</li></ol> <p>This loop continues until a stop condition is met which would commonly be an end-of-sequence token or a maximum token limit.</p> <h3>Output Control Techniques</h3> <h4>Greedy Sampling</h4> <p>Greedy sampling basically chooses the next token based on the highest probability it can achieve at every step.</p> <h4>Beam Search</h4> <p>Unlike greedy sampling, Beam search maintains multiple paths in parallel, expanding the most promising sequences. This helps the model to overlook more than just the next maximising token. This improves coherence at the cost of computation.</p> <h4>Temperature and Top-k/Nucleus Sampling</h4> <ul><li><strong>Temperature</strong> controls the randomness of sampling. This is applied as a scaling factor for the output logits before the softmax operation to control generation to be more deterministic or non-deterministic. Higher temperature values scale the distribution more and introduce a non-deterministic nature for next token selection and vice versa.</li><li><strong>Top-k sampling</strong> restricts choices to the k most likely tokens and randomly samples out of the top-k tokens.</li><li><strong>Nucleus (top-p) sampling</strong> chooses from the smallest set of tokens whose cumulative probability exceeds p.</li></ul> <h4>Length Penalty and Max Tokens</h4> <p>To prevent overly long or short generations, users can set maximum token limits and apply length penalties to the generation score.</p> <h4>Repetition Penalty</h4> <p>Models often repeat phrases. A repetition penalty modifies the logits of already-used tokens to discourage reuse.</p> <h3>Conclusion</h3> <p>Understanding how LLMs perform inference is vital for building efficient and responsive applications. The prefill and decode phases involve different trade-offs, from latency and memory to sampling and quality. With the right strategies like caching, batching, and careful sampling, you can harness the full potential of LLMs for your application needs.</p> <h3>References</h3> <ul><li>LLM inference: <a href="https://huggingface.co/learn/llm-course/chapter1/8?fw=pt#inference-with-llms">https://huggingface.co/learn/llm-course/chapter1/8?fw=pt#inference-with-llms</a></li></ul> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=36a767f98a83" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Long Tail Object Detection - How do we deal with it?</title><link href="https://lathashree01.github.io/blog/2025/long-tail-object-detection-how-do-we-deal-with-it/" rel="alternate" type="text/html" title="Long Tail Object Detection - How do we deal with it?"/><published>2025-06-06T17:48:51+00:00</published><updated>2025-06-06T17:48:51+00:00</updated><id>https://lathashree01.github.io/blog/2025/long-tail-object-detection---how-do-we-deal-with-it</id><content type="html" xml:base="https://lathashree01.github.io/blog/2025/long-tail-object-detection-how-do-we-deal-with-it/"><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*_UZmpGS82m6d23u_"/><figcaption><a href="https://unsplash.com/@cferdo?utm_source=medium&amp;utm_medium=referral">Fernando @cferdophotography</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure> <h3>Long tail Object Detection</h3> <p>In many real-world datasets, object classes follow a varied distribution, a few classes are very common, while many others appear infrequently. For example, you will see a huge amount of data on cats and dogs, but not a lot on specific species. Frequently occurring classes are called “head” and rare data classes are called “tail” classes. Performing object detection using such a dataset poses a set of challenges:</p> <ul><li>Models become biased towards head classes</li><li>Tail classes suffer from poor generalisation due to limited data</li></ul> <p>So, how can we improve feature learning so the model can perform well across the full spectrum of class frequencies?</p> <h3>Popular methods to mitigate Long-tailed object detection</h3> <p>The most popular ways to mitigate long-tailed object detection can be divided into two categories:</p> <h4>Data-level strategy</h4> <ol><li>Address the class imbalance by resampling — Under-sampling the majority classes and over-sampling the minority classes</li><li>Use open-source datasets to increase the minority classes</li><li>Increase the dataset by augmentations or generate synthetic datasets</li></ol> <h4>Model-level or Architecture-level strategy</h4> <ol><li>Add weightage to the loss function where you penalise the wrong predictions on minority classes more than the majority classes</li><li>Use a loss function that handles the class imbalance. E.g.. Focal loss</li><li>Leverage a pre-trained model backbone</li></ol> <p>These are really great strategies to address the class imbalance and enhance the model&#39;s learning. However, these pre-training paradigms leave some key detection components randomly initialised and tend to overlook the suboptimal performance issues caused by long-tailed distributions during the pre-training process.</p> <p>In one of the recent papers, I read a nice way to mitigate the long-tailed distribution targeting from the pretraining phase using contrastive learning.</p> <h4>Quick Refresher on Contrastive Learning</h4> <p><strong><em>Contrastive learning</em></strong> is a self-supervised approach that helps models learn useful representations by comparing data samples.</p> <p>The core idea is to bring similar samples (called positives) closer in the feature space while pushing dissimilar ones (negatives) apart. In the computer vision domain tasks, this often involves taking two augmented views of the same image as positives, and treating views from other images as negatives. This encourages the model to focus on underlying semantic features rather than low-level noise.</p> <p>It’s a foundational idea behind methods like SimCLR and MoCo, and has been widely adopted in pre-training for tasks such as classification and detection.</p> <h4>Long-Tailed Object Detection Pre-training: Dynamic Rebalancing Contrastive Learning with Dual Reconstruction</h4> <p>The paper stresses the importance of pretraining, as it plays a crucial role in object detection, especially when the data is imbalanced. It introduces a framework called Dynamic Rebalancing Contrastive Learning with Dual Reconstruction, which consists of three components: Holistic-Local Contrastive Learning, Dynamic Rebalancing, and Dual Reconstruction.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/618/1*p4wKf5I_oRZfR-Z9hyMiuQ.png"/><figcaption>Proposed method <a href="https://arxiv.org/pdf/2411.09453">paper</a></figcaption></figure> <h4>Holistic-Local Contrastive Learning (HLCL)</h4> <p>This module aims to pretrain both the backbone and the detection head by encouraging learning at both global (image) and local (object proposal) levels.</p> <ul><li>Holistic Contrastive Learning (HCL): Learns general visual features from the entire image using augmented views.</li><li>Local Contrastive Learning (LCL): Focuses on features from object proposals generated by a class-agnostic region proposal network.</li></ul> <h4>Dynamic Rebalancing</h4> <p>This part focuses on improving representation learning for tail classes by dynamically resampling the training data.</p> <p>Unlike traditional methods like Repeat Factor Sampling (RFS), this method looks at:</p> <ul><li>How often does each class appear across images (image-level)</li><li>How many object instances belong to each class (instance-level)</li></ul> <p>It calculates a harmonic mean of both to get a balanced repeat factor, which shifts over time to focus more on instance-level balancing as training progresses.</p> <h4>Dual Reconstruction</h4> <p>To counter simplicity bias (where models overly rely on shortcuts), this module encourages the model to learn both visual fidelity and semantic consistency.</p> <ul><li>Appearance Reconstruction: Reconstructs the original image from features using a pixel-wise loss.</li><li>Semantic Reconstruction: Applies a mask to the reconstructed image and ensures the features remain consistent with the original.</li></ul> <h4>Summary</h4> <p>By dynamically rebalancing training focus and enforcing both visual fidelity and semantic consistency, it significantly improves the quality of pre-trained representations. These enhancements lay a strong foundation for building object detectors that perform robustly across diverse and imbalanced datasets. It is an elegant and robust way to:</p> <ul><li>Align pretraining with downstream object detection</li><li>Adaptively rebalance class distributions</li><li>Preserve both low-level and high-level representations</li></ul> <h3>References</h3> <ul><li>Long-Tailed Object Detection Pre-training: Dynamic Rebalancing Contrastive Learning with Dual Reconstruction <a href="https://arxiv.org/pdf/2411.09453">https://arxiv.org/pdf/2411.09453</a></li><li>A wonderful blog on Contrastive Learning — <a href="https://lilianweng.github.io/posts/2021-05-31-contrastive/">https://lilianweng.github.io/posts/2021-05-31-contrastive/</a></li></ul> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c8666485f356" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Understanding the Types of Training for Large Language Models</title><link href="https://lathashree01.github.io/blog/2025/understanding-the-types-of-training-for-large-language-models/" rel="alternate" type="text/html" title="Understanding the Types of Training for Large Language Models"/><published>2025-06-01T12:56:42+00:00</published><updated>2025-06-01T12:56:42+00:00</updated><id>https://lathashree01.github.io/blog/2025/understanding-the-types-of-training-for-large-language-models</id><content type="html" xml:base="https://lathashree01.github.io/blog/2025/understanding-the-types-of-training-for-large-language-models/"><![CDATA[<h3>Understanding Types of training for LLMs</h3> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*OJc3QfmEJM3aMBSt"/><figcaption>Photo by <a href="https://unsplash.com/@tjerwin?utm_source=medium&amp;utm_medium=referral">Trent Erwin</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure> <h3>Pretraining, Fine-tuning, and Instruction Tuning Explained</h3> <p>The rise of Large Language Models (LLMs) like GPT, Gemini, and LLaMA has revolutionised natural language processing by enabling models to perform diverse tasks with minimal task-specific supervision. Central to their effectiveness is the <strong>multi-stage training process</strong> these models undergo, training phases such as <strong>pretraining</strong>, <strong>fine-tuning</strong>, and increasingly, <strong>instruction tuning </strong>for certain use cases where a user instruction needs to be followed.</p> <p>Understanding these stages is essential for aspiring ML engineers and organisations leveraging foundation models. Here’s a short breakdown of what each training phase entails, why it matters, and how they differ.</p> <h3>Pretraining: Learning the Foundations</h3> <p><strong>Objective:</strong> Equip the model with a general understanding of language, grammar, reasoning patterns, and world knowledge.</p> <p><strong>Approach:</strong><br/> Pretraining is carried out on vast amounts of <strong>unlabelled text</strong> using <strong>self-supervised learning</strong>. In the case of GPT-style models (decoder-only), this means <strong>causal language modelling</strong> — predicting the next token given the previous ones.</p> <p><strong>Data Sources:</strong></p> <ul><li>Books, articles, websites (e.g., Common Crawl)</li><li>Code (e.g., GitHub)</li><li>Multilingual corpora</li></ul> <p><strong>Outcome:</strong><br/> The model becomes a <strong>foundation model</strong> — a general-purpose LLM capable of understanding and generating natural language.</p> <blockquote>Example:<em> GPT-3 is pretrained on 300+ billion tokens from a wide range of internet sources, without any explicit task-specific labels.</em></blockquote> <h3>Post Training</h3> <h3>Fine-Tuning: Specialising for Specific Tasks</h3> <p><strong>Objective:</strong> Adapt the pretrained model to perform a <strong>specific downstream task</strong> such as sentiment analysis, named entity recognition, summarisation, or domain-specific QA.</p> <p><strong>Approach:</strong><br/>Fine-tuning uses <strong>supervised learning</strong> on <strong>labelled datasets</strong>, where the model is trained to map from inputs to expected outputs.</p> <ul><li>Often involves a smaller, task-specific dataset.</li><li>Can update all model parameters (full fine-tuning) or just a subset (e.g. adapters, LoRA, or QLoRA).</li></ul> <p><strong>Examples of Tasks:</strong></p> <ul><li>Text classification</li><li>Medical or legal domain QA</li><li>Multimodal extensions (e.g., VQA)</li></ul> <p><strong>Why it matters:</strong><br/> Fine-tuning enables customisation and improves performance in specialised contexts, making the model practical for real-world applications.</p> <blockquote>Example:<em> Fine-tuning GPT on financial documents to improve reasoning over balance sheets and contracts.</em></blockquote> <h3>Supervised Fine-tuning or Instruction Tuning: Making Models Follow Directions</h3> <p><strong>Objective:</strong> Teach the model to <strong>follow instructions</strong> and perform multiple tasks based on natural language prompts.</p> <p><strong>Approach:</strong><br/>Instruction tuning is a form of <strong>supervised fine-tuning</strong>, but instead of training on isolated tasks, the model is trained on a diverse set of <strong>instruction–response pairs</strong>.</p> <ul><li>Prompt: “Translate this sentence into French.”</li><li>Response: “Voici la phrase traduite en français.”</li></ul> <p>Instruction tuning is typically done after pretraining (and sometimes after fine-tuning), enabling models to generalise to unseen instructions.</p> <p><strong>Why it matters:</strong><br/> This step dramatically improves usability by aligning the model’s behaviour with human expectations, making it more helpful, reliable, and controllable.</p> <blockquote>Example:<em> InstructGPT outperforms GPT-3 on many NLP tasks despite being smaller, thanks to instruction tuning on prompt–response examples.</em></blockquote> <h3>Preference Tuning (PreFT)</h3> <p>A post-training technique that aligns the LLM to human preferences. Initially collect human preferences and use that data to align LLMs</p> <h3>Conclusion</h3> <p>The journey from raw text to task-aware intelligence involves strategically combining <strong>pretraining</strong>, <strong>fine-tuning</strong>, and <strong>instruction tuning</strong>. Each phase builds upon the previous one, starting from foundational knowledge, to specialised skills, and ultimately to usable, aligned behaviour.</p> <p>As models continue to scale and evolve, mastering these training paradigms is crucial for deploying LLMs safely and effectively in real-world systems.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c728ec8b205b" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">ML Model Deployment Strategies</title><link href="https://lathashree01.github.io/blog/2025/ml-model-deployment-strategies/" rel="alternate" type="text/html" title="ML Model Deployment Strategies"/><published>2025-04-24T23:01:39+00:00</published><updated>2025-04-24T23:01:39+00:00</updated><id>https://lathashree01.github.io/blog/2025/ml-model-deployment-strategies</id><content type="html" xml:base="https://lathashree01.github.io/blog/2025/ml-model-deployment-strategies/"><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*oECn3IvfJdTxfWk6"/><figcaption>Photo by <a href="https://unsplash.com/@lazizli?utm_source=medium&amp;utm_medium=referral">Lala Azizli</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure> <p>When most people think about machine learning, their minds immediately go to algorithms such as neural networks, decision trees, ensemble methods, and so on. While these are certainly important, real-world machine learning solutions involve much more than just model selection and training.</p> <p>One insightful paper illustrates this clearly, highlighting how the majority of effort in ML systems often lies outside the modelling phase. Data pipelines, infrastructure, monitoring, versioning, and deployment play equally, if not more, significant roles in making ML solutions viable in production environments.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*souEJG6g-of0YJaQ7idlxg.png"/><figcaption><a href="https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf">paper</a></figcaption></figure> <p>Among these, model deployment is a particularly critical stage in the ML lifecycle. Training accurate models is vital, but their value is only realised when they are reliably integrated into production systems and start serving real users at scale.</p> <p>This article outlines the key deployment strategies and discusses their trade-offs on selecting the right plan for your use case.</p> <h3><strong>Why Model Deployment Matters</strong></h3> <p>Model deployment is the bridge between building a machine learning model and deriving real-world value from it. It’s the stage where your trained model transitions from a development artefact to a production-grade service that interacts with real users or downstream systems.</p> <p>A robust deployment strategy ensures:</p> <ul><li><strong>Fast and reliable inference:</strong> Low-latency, high-availability serving of predictions, which is essential for delivering a seamless user experience.</li><li><strong>Safe rollout of updates:</strong> Enables controlled deployment of new models or features via techniques like canary releases and A/B testing, reducing the risk of service degradation.</li><li><strong>Continuous learning:</strong> Establishes feedback loops that support monitoring, drift detection, and retraining, which is crucial for maintaining performance in dynamic environments.</li></ul> <h3><strong>Common Model Deployment Strategies</strong></h3> <p>Each deployment strategy comes with its own set of strengths, trade-offs, and implementation complexity. Your choice should align with the specific goals of the system, whether it’s minimising risk, ensuring rapid iteration, or enabling experimentation at scale.</p> <p>Below are some of the most widely adopted deployment methods in the industry today.</p> <h4><strong>Canary Deployment</strong></h4> <p>In a canary deployment, a new model version is initially exposed to a small fraction of production traffic, typically 5–10%. If the new version performs well, traffic is incrementally increased until it serves all requests. If regressions are detected, the deployment can be rolled back swiftly.</p> <p>Canary deployment enables a safe, gradual rollout by exposing the new model to a small fraction of users before full release. Its main advantages are risk mitigation and the ability to quickly roll back if issues arise. However, it demands detailed monitoring to catch regressions early and requires more complex routing logic to manage traffic splits effectively.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*3We-Rbm40C4fy6ozxnXLdw.png"/><figcaption>Direct a small proportion of your user traffic to new model (challenger model)</figcaption></figure> <h4><strong>Shadow Deployment</strong></h4> <p>In shadow deployment, the new model runs silently alongside the production model, processing the same real-time inputs but without serving its outputs to users. Instead, its predictions are logged and compared with the current production model, enabling safe validation in a live environment.</p> <p>This strategy offers the advantage of zero user impact while allowing for real-time performance evaluation, making it ideal for de-risking major changes. However, it comes at the cost of additional infrastructure requirements, logging and monitoring setups, and most importantly, doesn’t capture how users might respond to the new model’s outputs, limiting its ability to assess user-facing impact directly.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*pxoF9226x5HHcvbmKzgmjQ.png"/><figcaption>The <em>challenger</em> model receives the same user requests as the production model, with its predictions logged and analysed to assess performance.</figcaption></figure> <h4><strong>Blue-Green Deployment</strong></h4> <p>Blue-green deployment involves running two identical production environments: “blue”, which serves the current stable model, and “green”, which hosts the new version. Once the new model in the green environment has been fully validated, production traffic is switched over in a single step, ensuring a smooth transition.</p> <p>This strategy enables instant switching with zero downtime, making it particularly suitable for systems with strict uptime requirements. Rollbacks are also straightforward, where they simply route traffic back to the blue environment if any issues emerge post-deployment.</p> <p>However, the approach comes with the operational cost of running duplicate infrastructure, a big tradeoff when you want to maintain strict SLAs for the whole system.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*SzZ9ZCDo6EtoTa-kgkw7oQ.png"/><figcaption>Replicate the complete system with the challenger model in the green environment. Once everything is validated and ready, switch traffic from blue to green. If successful, the green environment is promoted to blue. In case of failure, a rollback can be performed by reverting traffic to the original blue.</figcaption></figure> <h3>Choosing the Right Deployment Strategy</h3> <p>Selecting the appropriate deployment strategy depends on several key factors:</p> <ul><li><strong>Risk tolerance:</strong> For high-risk systems, such as those in healthcare or finance, shadow or canary deployments provide safer, low-impact validation.</li><li><strong>Experimentation needs:</strong> If the goal is to test model variants and measure business impact, A/B testing is ideal for running controlled experiments in production.</li><li><strong>Infrastructure maturity:</strong> Strategies like blue-green require the ability to duplicate production environments, which may not be feasible in all setups and incur additional infrastructure costs.</li><li><strong>User impact:</strong> If minimising exposure to unstable models is a priority, shadow or blue-green deployments help reduce the risk to end users.</li><li><strong>Monitoring capabilities:</strong> Canary and A/B deployments require strong observability and analytics setups to detect degradation and evaluate performance under live traffic.</li></ul> <p>In practice, organisations often start with simpler approaches such as manual rollouts or basic blue-green setups, and gradually adopt more sophisticated strategies as their infrastructure, team expertise, and monitoring systems mature.</p> <p><strong>Coming up: </strong>In the next articles, I am planning to dive deeper into packaging ML models with Docker and deploying them in real-world environments. Stay tuned for practical guides and hands-on insights.</p> <h3><strong>Conclusion</strong></h3> <p>Model deployment is where machine learning solutions begin to deliver real-world value. By choosing the right deployment strategy and implementing sound engineering practices, organisations can ensure their models are scalable, reliable, and adaptable to change. As ML systems become increasingly complex, adopting robust deployment strategies is no longer just a best practice; it’s a necessity.</p> <p><em>What deployment strategy do you use in your ML systems? Let me know in the comments.</em></p> <h3>References:</h3> <ul><li>Hidden Technical Debt in Machine Learning Systems <a href="https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf">https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf</a></li></ul> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ea38bad3fe82" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Hidden Technical Debt in Machine Learning Systems</title><link href="https://lathashree01.github.io/blog/2025/hidden-technical-debt-in-machine-learning-systems/" rel="alternate" type="text/html" title="Hidden Technical Debt in Machine Learning Systems"/><published>2025-02-27T23:56:53+00:00</published><updated>2025-02-27T23:56:53+00:00</updated><id>https://lathashree01.github.io/blog/2025/hidden-technical-debt-in-machine-learning-systems</id><content type="html" xml:base="https://lathashree01.github.io/blog/2025/hidden-technical-debt-in-machine-learning-systems/"><![CDATA[<blockquote>Exploring the unseen costs of building and maintaining ML systems — and how to mitigate them.</blockquote> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*F0MzBt-GHz7ZIAkK"/><figcaption><a href="https://unsplash.com/@karishea?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure> <p>Machine learning has revolutionised how we solve complex problems — from personalised recommendations to autonomous driving and many more. Yet, while rapid prototyping and deployment may seem like a quick win, they often come with a hidden price tag: <strong><em>Technical Debt</em></strong>. Drawing inspiration from the seminal paper “Hidden Technical Debt in Machine Learning Systems” by Sculley et al., this article delves into the unique challenges of ML technical debt along with actionable strategies for mitigating long-term maintenance costs.</p> <h3><em>What is Technical Debt in ML?</em></h3> <p>Let us first understand what we mean by technical debt in machine learning systems.</p> <p>In the software development field, <strong>technical debt</strong> (also known as <strong>design debt </strong>or <strong>code debt</strong>) refers to the implied cost of additional work in the future resulting from choosing an expedient solution over a more robust one. [<a href="https://en.wikipedia.org/wiki/Technical_debt">Wikipedia</a>]</p> <p>In ML systems, however, the same problem is compounded by various problems such as:</p> <ul><li><strong>Entanglement:</strong> Small changes in input data or features can unexpectedly ripple through the system.</li><li><strong>Data Dependencies:</strong> Unstable or redundant data features add complexity and risk.</li><li><strong>Configuration Complexity:</strong> An ever-growing number of configuration settings can lead to brittle systems.</li></ul> <p>This debt may be difficult to detect because it exists at the system level rather than the code level. Only a small fraction of real-world ML systems are composed of ML code, as shown by the small black box in the middle. The required surrounding infrastructure is vast and complex. Hence, understanding these issues is critical as they not only affect model performance but also hinder the long-term maintainability and scalability of the entire ML system.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*P8kJgYx2yeqqgvE7pTG5QA.png"/><figcaption>Real-world ML system components (source: 1)</figcaption></figure> <h3>Understanding the main causes of ML Tech Debt</h3> <h3>1. Entanglement and the CACE Principle</h3> <p>The “Changing Anything Changes Everything” (CACE) principle reflects how interconnected components in ML systems can be. A minor tweak in one part of the pipeline may trigger unexpected changes elsewhere. For example, changing the input data distribution of some features can cause a change in the importance of other features used in ML systems. This could also apply to many other hyperparameters used in the model training process.</p> <p><strong>Actionable Strategies to mitigate this:</strong></p> <ul><li><strong>Isolate Components:</strong> Design your model components to be independent, if possible.</li><li><strong>Monitor Input Distributions:</strong> Implement continuous monitoring and slice-based metrics to detect when the model’s prediction behaviour changes; this is a good indicator that something in the input has changed (unless the model itself has changed).</li></ul> <h3>2. Undeclared Consumers</h3> <p>ML model predictions can often be left accessible to other systems. Having no visibility of who consumes the output of the model is risky because changes in the ML model affect the downstream processes that are consuming it. This can also be called “Visibility debt”.</p> <p><strong>Actionable Strategies to mitigate this:</strong></p> <ul><li><strong>Document Consumers:</strong> Maintain a registry of all systems that consume your model outputs.</li><li><strong>Access Controls:</strong> Implement strict access policies and service-level agreements (SLAs) to manage data usage.</li></ul> <h3>3. Data Dependencies</h3> <p>Data is the lifeblood of any ML system. However, unstable or underutilised data signals can become significant liabilities more than the code dependencies.</p> <p><strong>Actionable Strategies to mitigate this:</strong></p> <ul><li><strong>Versioning:</strong> Freeze versions of critical data features to prevent sudden, detrimental changes.</li><li><strong>Feature Audits to remove underutilised dependencies:</strong> Conduct regular leave-one-feature-out evaluations to identify and retire redundant or low-value features.</li><li><strong>Automated Dependency Tracking:</strong> Use tooling to annotate and validate the relationships between features.</li></ul> <h3>4. Configuration Debt and Pipeline Jungles</h3> <p>Over time, configurations and data pipelines can become so chucky. Having misconfigured pipelines is expensive in terms of wasted effort money and unnecessary delays. We should always think that a highly configurable system is more mature because there is less room for errors. Let us think about the principles of good configuration systems.</p> <ul><li>Easy to specify a configuration and modify from the previous version</li><li>It should be hard to make manual errors in the configuration. It&#39;s absolutely necessary to validate the configuration values!</li><li>Detect unused and redundant settings left from previous iterations</li><li>It should be part of the code repository and versioned like other code.</li></ul> <h3>5. Changes in the External World</h3> <p>Changes in the external world, such as shifts in data sources, regulatory requirements, or technological landscapes, can cause your models to not work as well as they used to. This can affect how reliable and accurate your predictions are.</p> <p><strong>Actionable Strategies to mitigate this:</strong></p> <h4><strong>Model Monitoring:</strong></h4> <ul><li><strong>Why it’s important: </strong>Monitoring is like keeping a close eye on your model after it’s been put to work. It helps you see if it’s still performing well over time.</li><li><strong>How to do it:</strong> Set up dashboards to track key performance indicators (KPIs), data drift, and system anomalies. Think of KPIs as vital signs for your model.</li><li><strong>Data Drift:</strong> Data drift is when the data your model is seeing in the real world starts to look different from the data it was trained on.</li><li><strong>Why retrain: </strong>Monitoring helps you decide when to retrain your model with new data. Models can become “stale” if they’re not updated, which can lead to less accurate results.</li></ul> <h4><strong>End-to-End Testing:</strong></h4> <ul><li><strong>What it is:</strong> This involves testing the entire data flow, from where the data comes in (ingestion) to the final prediction.</li><li><strong>Why it’s important:</strong> Before putting a new model into production, you want to make sure it’s better than the one you’re already using.</li><li><strong>How to do it:</strong> Develop comprehensive tests that simulate the entire data flow.</li></ul> <p>There are lots of other causes for ML tech debt that are mentioned in the paper; I chose to explain some of the causes that strongly resonate with my experience. Please look into the reference section to learn more about other cases.</p> <h3>Practical Guidelines for Practitioners</h3> <ul><li>Practice data validation and check for data leakage between your training and validation</li><li>Version critical data features to track changes.</li><li>Isolate model components using a modular design.</li><li>Document your customers, specifying who your model’s direct consumers are.</li><li>Audit code &amp; config regularly for security and dependency updates.</li><li>Maintain a model registry like MLflow or Weights &amp; Biases.</li><li>Ensure reproducibility of model deployments.</li><li>Use gradual rollouts: canary, A/B testing. I wrote an article on model deployments, check it out: <a href="https://lathashreeh.medium.com/deploying-machine-learning-models-in-production-ea38bad3fe82">Deploying Machine Learning Models in Production</a></li><li>Monitor and test ML systems, and make sure to have alerting systems.</li><li>Production model degrades, decide on a retraining period and update the model regularly.</li><li>Last but not least, use diagrams to visualise your dataflow and the entire system design. Don&#39;t underestimate the need for good documentation :p</li></ul> <h3>Conclusion and Future Outlook</h3> <p>Managing technical debt in ML systems is an ongoing challenge. Recognising the hidden costs early and implementing strategies to mitigate them can save considerable time and resources in the long run. As ML systems continue to grow in complexity, the development of standardised tools and frameworks will be crucial. I encourage ML practitioners to take a proactive stance — audit your systems, simplify where possible, and share your learnings with the community.</p> <p><em>What steps have you taken to manage technical debt in your ML projects? <br/>Share your experiences in the comments below, and let’s build a more maintainable future for machine learning together.</em></p> <h3>References:</h3> <ol><li>D. Sculley et. al., Hidden Technical Debt in Machine Learning Systems, <a href="https://proceedings.neurips.cc/paper_files/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf</a></li></ol> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=27fa1b13040c" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">LLM evaluation strategies and methods</title><link href="https://lathashree01.github.io/blog/2025/llm-evaluation-strategies-and-methods/" rel="alternate" type="text/html" title="LLM evaluation strategies and methods"/><published>2025-02-05T00:18:38+00:00</published><updated>2025-02-05T00:18:38+00:00</updated><id>https://lathashree01.github.io/blog/2025/llm-evaluation-strategies-and-methods</id><content type="html" xml:base="https://lathashree01.github.io/blog/2025/llm-evaluation-strategies-and-methods/"><![CDATA[<figure><img alt="Human-machine interaction" src="https://cdn-images-1.medium.com/max/848/1*Wi1yq-hZJH4ElnXhSYVtjA.jpeg"/><figcaption><a href="https://www.simplilearn.com/machine-learning-vs-deep-learning-major-differences-you-need-to-know-article">source</a></figcaption></figure> <p>In the rapidly advancing field of AI, we are witnessing exponential advancements that are reshaping technology at a remarkable pace. The emergence of ChatGPT marked a pivotal moment in AI, sparking a global conversation about language models capable of performing tasks with exceptional proficiency, far beyond what was previously imagined !!</p> <blockquote>Artificial Intelligence is not a substitute for human intelligence; it is a tool to amplify human creativity and ingenuity. — Fei Fei Li</blockquote> <p>This article explores types of use cases that you can build using LLMs and evaluation strategies for understanding LLMs’ performance.</p> <p><strong><em>A quick recap for someone wondering what an LLM is</em></strong></p> <p>LLM is an advanced version of a language model with billions of parameters, which would be trained on vast amounts of text data. LLMs excel at both <strong>generative</strong> tasks, such as writing coherent paragraphs, and <strong>predictive</strong> tasks, like filling in missing words in sentences or classifying texts.</p> <h3><strong><em>What can we build using LLMs?</em></strong></h3> <p>Applications that integrate LLMs are often referred to as <strong>LLM-powered applications</strong>, enabling capabilities such as chatbots like ChatGPT, automated content generation, and agentic solutions. When we build such an application, it is very critical to ensure it is stable and tested thoroughly.</p> <p>LLM outputs are non-deterministic, meaning the model can generate different responses to the same input. Given this behaviour, evaluation criteria should focus on understanding both its capabilities and potential risks. For example, if we are building a Question &amp; Answering System, it is essential to check whether the chatbot is giving correct answers. Some other evaluation criteria could include — Are the answers helpful? Is the tone okay? Is the model hallucinating? Is the chatbot being fair? etc.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/693/1*w0PrITi8J_7Bu6t47VwnGQ.png"/></figure> <h3><strong><em>How do we evaluate our LLM apps?</em></strong></h3> <p>There are several ways to evaluate LLMs, including manual evaluation and automated methods. One approach is to assemble a set of domain experts to create a gold-standard dataset and compare the model’s output against the experts’ expectations. However, this process can be costly and may not be feasible when building solutions for certain specialised domains, such as healthcare.</p> <p>Let’s explore different approaches for gathering evaluation datasets and the strategies you can use to assess LLM performance.</p> <p><strong><em>Ways to generate Evaluation datasets:</em></strong></p> <ul><li><strong>Using Publicly Available Benchmark Datasets</strong> — Leverage established benchmark datasets designed for evaluating LLMs.</li><li><strong>Utilising Existing Business Data</strong> — Usage of internal datasets relevant to your specific domain.</li><li><strong>Generating Synthetic Datasets</strong> — Create artificial data using existing source documents.</li><li><strong>Creating Test Cases</strong> — Design targeted test cases to assess model responses based on predefined evaluation criteria.</li></ul> <p><strong><em>By combining all the evaluation datasets, we can group the final evaluation strategy into three main categories:</em></strong></p> <ol><li><strong>Happy Path</strong><br/>Covers expected use cases where the application functions as intended, ensuring the model performs well in typical scenarios.</li><li><strong>Edge Cases</strong><br/>Includes unexpected user interactions that fall outside the project scope. For example, how does the model handle off-topic questions or sensitive queries?</li><li><strong>Adversarial Scenarios</strong><br/>Focuses on attempts by malicious users to exploit the application, such as extracting harmful content or manipulating the LLM’s responses.</li></ol> <h3><strong><em>LLM Evaluation Methods and Metrics</em></strong></h3> <p><strong><em>Group 1:</em> Requires Ground Truth Dataset — Measures Correctness</strong></p> <p>These evaluation methods are focused on assessing the correctness of the model’s outputs, typically by comparing them to a predefined “ground truth” or expected outcome.</p> <ol><li><strong>Predictive Quality Metrics</strong><br/>Predictive quality metrics are primarily used for tasks like classification, where the goal is to predict labels based on input data. Common metrics in this category include:</li></ol> <ul><li><strong>Accuracy</strong>: The percentage of correct predictions out of all predictions made.</li><li><strong>Precision, Recall, and F1-score</strong>: Precision measures the proportion of correct positive predictions out of all positive predictions, while recall calculates the proportion of correct positive predictions out of all actual positives. The F1-score is the harmonic mean of precision and recall, providing a balance between the two.</li><li><strong>Area Under the Receiver Operating Characteristic Curve (AUC-ROC)</strong>: This measures the model’s ability to distinguish between classes across different thresholds.</li></ul> <p><strong>2. Generative Quality Metrics</strong><br/>In generative tasks, such as text generation or machine translation, the model is expected to produce fluent and accurate outputs. Common metrics include:</p> <ul><li><strong>BLEU (Bilingual Evaluation Understudy)</strong>: Measures the overlap of n-grams between the generated text and the reference text. It’s widely used in machine translation.</li><li><strong>ROUGE (Recall-Oriented Understudy for Gisting Evaluation)</strong>: Measures the overlap of n-grams, words, or word sequences between the model-generated output and a set of reference outputs. It’s often used in tasks like summarisation.</li></ul> <p><strong>3. Semantic Similarity</strong><br/>These methods assess how semantically similar the model-generated output is to the reference output at the embedding level. The most common approach is to measure the cosine similarity between the embeddings of the generated and reference texts. A higher cosine similarity indicates that the two pieces of text are more similar in meaning.</p> <ul><li><strong>Cosine Similarity</strong>: Measures the cosine of the angle between two vectors (representing text in an embedding space). A cosine similarity of 1 indicates identical vectors, while 0 indicates orthogonal and -1 indicates opposite (completely dissimilar) vectors.</li></ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/768/0*Flk84kQHxk22B-Wk.png"/><figcaption><a href="https://kdb.ai/learning-hub/articles/methods-of-similarity/">Source</a></figcaption></figure> <p><strong>4. LLM-as-a-Judge</strong><br/>This method involves the LLM acting as the judge or evaluator of the potential LLM’s responses, even without a strict ground truth.</p> <p>For example, the model could assess its output based on predefined guidelines or criteria, such as coherence, relevance, and alignment with the expected output.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*hgK4CjylQhWOU3I9vZ-HOg.png"/><figcaption><a href="https://www.evidentlyai.com/">Source</a></figcaption></figure> <blockquote>Sometimes a bigger and better LLM is used as a judge to evaluate the candidate LLM’s outputs.</blockquote> <p><strong><em>Group 2:</em> Does Not Require Ground Truth — Measures Relevance</strong></p> <p>These methods evaluate how relevant, useful, or meaningful the model’s output is, without requiring an exact “correct” answer to compare against.</p> <ol><li><strong>Sentiment Analysis</strong><br/>Sentiment analysis evaluates the emotional tone or sentiment behind a piece of text. LLMs can be evaluated by how accurately they identify sentiments such as positive, negative, or neutral within generated content. While this doesn’t require ground truth in the traditional sense, it relies on established sentiment categories to determine relevance and accuracy.</li><li><strong>Regular Expressions</strong><br/>Regular expressions can be used to evaluate specific patterns or structures within the generated text, such as verifying the presence of phone numbers, email addresses, or dates in a response. This type of evaluation does not depend on ground truth, but rather on whether the output meets certain predefined patterns or formats.</li><li><strong>LLM-as-a-Judge (Relevance Evaluation)</strong><br/>Here, the LLM evaluates its response for relevance, usefulness, and alignment with the task. For instance, the model could assess whether its answer is on-topic and provides valuable information. This approach doesn’t require a ground truth but relies on predefined relevance criteria.</li><li><strong>Functional Testing</strong><br/>Functional testing ensures that LLM-generated outputs are executable and usable in real-world contexts. Examples include:</li></ol> <ul><li><strong>Executing Code</strong>: Running generated code (e.g., Python) to verify it works as intended.</li><li><strong>SQL Query Execution</strong>: Testing generated SQL queries by executing them against a database to ensure they return the expected results.</li><li><strong>APIs and Workflows</strong>: Verifying that generated API calls or workflows function as intended in a practical application.</li></ul> <p>The focus is on the relevance and operability of the model’s outputs.</p> <p>In conclusion, evaluating LLM applications requires careful examination by combining methods that assess both the correctness and relevance of LLM’s outputs. By using techniques such as ground-truth-based metrics, functional testing, and self-assessment by another LLM, we can gain a comprehensive understanding of the model’s performance. Thorough evaluation is crucial to ensure that LLMs not only generate accurate outputs but also remain useful, relevant, and reliable in real-world applications.</p> <h3><em>References:</em></h3> <ol><li>LLM evaluation course by Evidently AI <a href="https://www.evidentlyai.com/llm-evaluations-course">https://www.evidentlyai.com/llm-evaluations-course</a></li><li>What are LLMs? by IBM <a href="https://www.ibm.com/think/topics/large-language-models">https://www.ibm.com/think/topics/large-language-models</a></li></ol> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=1704771ed7e8" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Significance of Interpretability in Machine Learning: Unveiling the ‘Why’ Behind Predictions</title><link href="https://lathashree01.github.io/blog/2023/significance-of-interpretability-in-machine-learning-unveiling-the-why-behind-predictions/" rel="alternate" type="text/html" title="Significance of Interpretability in Machine Learning: Unveiling the ‘Why’ Behind Predictions"/><published>2023-10-28T19:56:07+00:00</published><updated>2023-10-28T19:56:07+00:00</updated><id>https://lathashree01.github.io/blog/2023/significance-of-interpretability-in-machine-learning-unveiling-the-why-behind-predictions</id><content type="html" xml:base="https://lathashree01.github.io/blog/2023/significance-of-interpretability-in-machine-learning-unveiling-the-why-behind-predictions/"><![CDATA[<blockquote>In the ever-evolving field of machine learning, the importance of model performance is undisputed. However, relying solely on performance metrics like accuracy can be misleading, as they offer an incomplete description of real-world tasks. In this article, I delve into the vital role of interpretability in machine learning models, exploring why understanding the ‘why’ behind predictions is of utmost importance.</blockquote> <figure><img alt="" src="https://cdn-images-1.medium.com/max/685/1*M3BQ5LP5vy4g8U2rcC6x_Q.png"/><figcaption>Source: <a href="https://www.nature.com/articles/s41929-022-00744-z">link</a></figcaption></figure> <h3>Introduction</h3> <p>Machine learning models have transformed numerous industries, from healthcare to finance, by making accurate predictions and automating decision-making processes. However, a myopic focus on model performance metrics can lead to unintended consequences, especially when human lives and ethical concerns are at stake. Hence exploring the significance of interpretability in machine learning and how it influences the ‘what’ and ‘why’ of model decisions is extremely important.</p> <h3>The Dilemma: <em>What vs. Why</em></h3> <p>The core dilemma in machine learning is whether one should be content with knowing ‘what’ a model predicts or delve deeper into ‘why’ a particular prediction was made. The ‘what’ primarily encompasses the outcome — whether a customer will churn or the effectiveness of a drug for a patient. While knowing the ‘what’ is essential, the ‘why’ is equally crucial in various contexts.</p> <h3>Safety and Trust</h3> <p>Interpretability becomes imperative in high-risk applications such as self-driving cars. Understanding ‘why’ the car made a specific decision, like detecting a cyclist, can prevent accidents and build trust in autonomous systems. It’s not just about knowing the outcome; it’s about comprehending the underlying reasoning.</p> <h3>Fairness and Bias</h3> <p>Machine learning models can inadvertently perpetuate bias, discrimination, and unfairness. Interpretability serves as a powerful tool for detecting and mitigating these issues. By understanding ‘why’ a model made a decision, it becomes easier to identify and rectify learned biases. This is especially critical in scenarios like credit approval, where fairness is paramount.</p> <h3>Human Understanding and Learning</h3> <p>Humans are naturally curious and seek explanations for unexpected events and contradictions. Interpretability caters to this human curiosity and learning process. It aligns machine behaviour with human expectations, making it easier for users to trust and comprehend machine decisions.</p> <h3>When Interpretability is Not Required</h3> <p>While interpretability holds significant value, there are scenarios where it may not be necessary. In low-impact applications or domains that have been extensively studied, the focus may shift more towards model performance than understanding the ‘why.’ Nevertheless, it’s essential to evaluate each situation on a case-by-case basis.</p> <h3>Risks of Interpretability</h3> <p>Interpretability can potentially introduce risks, particularly when users manipulate the system based on their understanding of the model’s behaviour. Striking a balance between transparency and security is crucial to prevent misuse.</p> <h3>Conclusion</h3> <p>In a world increasingly driven by machine learning models, interpretability emerges as an essential facet of AI ethics and responsible AI deployment. It empowers us to comprehend and trust machine decisions, detect and rectify biases, and ensure the safety of applications in various domains. While it may not be universally required, the significance of interpretability becomes evident when the impact and consequences of model decisions are substantial. In this rapidly evolving field, understanding ‘why’ is as crucial as knowing ‘what.’</p> <p><strong><em>References:</em></strong></p> <pre>1. Molnar, C. (2022). Interpretable Machine Learning:<br />A Guide for Making Black Box Models Explainable (2nd ed.).<br />christophm.github.io/interpretable-ml-book/</pre> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=7bf7dadc63a0" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry></feed>