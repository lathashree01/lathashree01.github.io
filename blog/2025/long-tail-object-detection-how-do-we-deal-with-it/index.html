<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*_UZmpGS82m6d23u_"><figcaption><a href="https://unsplash.com/@cferdo?utm_source=medium&amp;utm_medium=referral" rel="external nofollow noopener" target="_blank">Fernando @cferdophotography</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="external nofollow noopener" target="_blank">Unsplash</a></figcaption></figure> <h3>Long tail Object Detection</h3> <p>In many real-world datasets, object classes follow a varied distribution, a few classes are very common, while many others appear infrequently. For example, you will see a huge amount of data on cats and dogs, but not a lot on specific species. Frequently occurring classes are called “head” and rare data classes are called “tail” classes. Performing object detection using such a dataset poses a set of challenges:</p> <ul> <li>Models become biased towards head classes</li> <li>Tail classes suffer from poor generalisation due to limited data</li> </ul> <p>So, how can we improve feature learning so the model can perform well across the full spectrum of class frequencies?</p> <h3>Popular methods to mitigate Long-tailed object detection</h3> <p>The most popular ways to mitigate long-tailed object detection can be divided into two categories:</p> <h4>Data-level strategy</h4> <ol> <li>Address the class imbalance by resampling — Under-sampling the majority classes and over-sampling the minority classes</li> <li>Use open-source datasets to increase the minority classes</li> <li>Increase the dataset by augmentations or generate synthetic datasets</li> </ol> <h4>Model-level or Architecture-level strategy</h4> <ol> <li>Add weightage to the loss function where you penalise the wrong predictions on minority classes more than the majority classes</li> <li>Use a loss function that handles the class imbalance. E.g.. Focal loss</li> <li>Leverage a pre-trained model backbone</li> </ol> <p>These are really great strategies to address the class imbalance and enhance the model's learning. However, these pre-training paradigms leave some key detection components randomly initialised and tend to overlook the suboptimal performance issues caused by long-tailed distributions during the pre-training process.</p> <p>In one of the recent papers, I read a nice way to mitigate the long-tailed distribution targeting from the pretraining phase using contrastive learning.</p> <h4>Quick Refresher on Contrastive Learning</h4> <p><strong><em>Contrastive learning</em></strong> is a self-supervised approach that helps models learn useful representations by comparing data samples.</p> <p>The core idea is to bring similar samples (called positives) closer in the feature space while pushing dissimilar ones (negatives) apart. In the computer vision domain tasks, this often involves taking two augmented views of the same image as positives, and treating views from other images as negatives. This encourages the model to focus on underlying semantic features rather than low-level noise.</p> <p>It’s a foundational idea behind methods like SimCLR and MoCo, and has been widely adopted in pre-training for tasks such as classification and detection.</p> <h4>Long-Tailed Object Detection Pre-training: Dynamic Rebalancing Contrastive Learning with Dual Reconstruction</h4> <p>The paper stresses the importance of pretraining, as it plays a crucial role in object detection, especially when the data is imbalanced. It introduces a framework called Dynamic Rebalancing Contrastive Learning with Dual Reconstruction, which consists of three components: Holistic-Local Contrastive Learning, Dynamic Rebalancing, and Dual Reconstruction.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/618/1*p4wKf5I_oRZfR-Z9hyMiuQ.png"><figcaption>Proposed method <a href="https://arxiv.org/pdf/2411.09453" rel="external nofollow noopener" target="_blank">paper</a></figcaption></figure> <h4>Holistic-Local Contrastive Learning (HLCL)</h4> <p>This module aims to pretrain both the backbone and the detection head by encouraging learning at both global (image) and local (object proposal) levels.</p> <ul> <li>Holistic Contrastive Learning (HCL): Learns general visual features from the entire image using augmented views.</li> <li>Local Contrastive Learning (LCL): Focuses on features from object proposals generated by a class-agnostic region proposal network.</li> </ul> <h4>Dynamic Rebalancing</h4> <p>This part focuses on improving representation learning for tail classes by dynamically resampling the training data.</p> <p>Unlike traditional methods like Repeat Factor Sampling (RFS), this method looks at:</p> <ul> <li>How often does each class appear across images (image-level)</li> <li>How many object instances belong to each class (instance-level)</li> </ul> <p>It calculates a harmonic mean of both to get a balanced repeat factor, which shifts over time to focus more on instance-level balancing as training progresses.</p> <h4>Dual Reconstruction</h4> <p>To counter simplicity bias (where models overly rely on shortcuts), this module encourages the model to learn both visual fidelity and semantic consistency.</p> <ul> <li>Appearance Reconstruction: Reconstructs the original image from features using a pixel-wise loss.</li> <li>Semantic Reconstruction: Applies a mask to the reconstructed image and ensures the features remain consistent with the original.</li> </ul> <h4>Summary</h4> <p>By dynamically rebalancing training focus and enforcing both visual fidelity and semantic consistency, it significantly improves the quality of pre-trained representations. These enhancements lay a strong foundation for building object detectors that perform robustly across diverse and imbalanced datasets. It is an elegant and robust way to:</p> <ul> <li>Align pretraining with downstream object detection</li> <li>Adaptively rebalance class distributions</li> <li>Preserve both low-level and high-level representations</li> </ul> <h3>References</h3> <ul> <li>Long-Tailed Object Detection Pre-training: Dynamic Rebalancing Contrastive Learning with Dual Reconstruction <a href="https://arxiv.org/pdf/2411.09453" rel="external nofollow noopener" target="_blank">https://arxiv.org/pdf/2411.09453</a> </li> <li>A wonderful blog on Contrastive Learning — <a href="https://lilianweng.github.io/posts/2021-05-31-contrastive/" rel="external nofollow noopener" target="_blank">https://lilianweng.github.io/posts/2021-05-31-contrastive/</a> </li> </ul> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c8666485f356" width="1" height="1" alt=""></p> </body></html>