<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*80b4bJAk8gU8EC4Z"><figcaption>Photo by <a href="https://unsplash.com/@dbeamer_jpg?utm_source=medium&amp;utm_medium=referral" rel="external nofollow noopener" target="_blank">Drew Beamer</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="external nofollow noopener" target="_blank">Unsplash</a></figcaption></figure> <p>LLMs have revolutionised natural language understanding and generation. Whether you are building chatbots, drafting emails, or generating code, a trained model runs in the backend to produce outputs from users’ inputs. However, the process of inference is more nuanced than “feed in a prompt and receive text.”</p> <p>In this article, we will explore the two main stages of autoregressive LLM inference, namely as <strong>Prefill</strong> phase and the <strong>Decode</strong> phase. I will also delve into practical techniques for controlling output length, mitigating repetitiveness, and leveraging beam search.</p> <p>By the end, you should have a better technical understanding of how LLMs generate text and how you can steer them towards desired behaviours.</p> <h3>Overview of Autoregressive Inference</h3> <p>Most recent LLMs (e.g. GPT-style models) are autoregressive, meaning they generate text one token at a time, conditioning on all previously generated tokens.</p> <p>At a high level, the process has two phases:</p> <ol> <li> <strong>Prefill (Prompt Encoding):</strong> The model processes the user’s prompt in full, producing hidden states (and typically key-value caches) that summarise the prompt’s content.</li> <li> <strong>Decode (Autoregressive Generation):</strong> The model enters a loop, where at each step it takes the hidden state of the tokens generated so far, predicts a distribution over the next token, samples or selects one, appends it to the sequence, and updates its hidden state (via cached keys/values).</li> </ol> <p>Understanding the distinction between these phases is crucial for optimising latency and for implementing decoding strategies that influence output quality.</p> <h3>Prefill Phase</h3> <p>When you supply a prompt, say, “Once upon a time, in a distant kingdom…” the model must convert each token in that input text into internal representations. This involves several steps:</p> <h4>Tokenisation</h4> <p>The input text is split into units called <em>tokens</em>, which are then encoded to obtain corresponding numbers (called <em>token_id) </em>for the substrings from a predefined vocabulary.</p> <h4>Embedding Lookup</h4> <p>Each <em>token_id</em> is mapped to a dense vector (<em>token embedding</em>), and a <em>positional embedding</em> is added to capture the positional information of each token in the sequence.</p> <h4>Attention Mechanism</h4> <p>The embedded tokens are processed by a stack of transformer blocks, each comprising:</p> <ul> <li> <strong>Self-attention</strong> enables each token to attend to all others in the input sequence, capturing contextual relationships.</li> <li> <strong>Feed-forward networks</strong> apply nonlinear transformations to each token.</li> </ul> <h4>Key-Value Cache Construction</h4> <p>In decoder-only architectures, each transformer layer outputs <em>key</em> and <em>value</em> matrices used in future attention computations. Caching these avoids recomputing attention from scratch at every decode step.</p> <p>Once the prefill is done, the model holds a “snapshot” of hidden states and caches representing the entire input sequence.</p> <h3>Practical Considerations</h3> <h4>Latency vs. Throughput</h4> <ul> <li>Prefill is a one-time cost per prompt.</li> <li>Decode occurs once per generated token.</li> <li>For long completions, caching substantially reduces compute; for short completions, the prefill overhead might dominate.</li> </ul> <h4>Batching</h4> <ul> <li>Multiple prompts can be processed in parallel during prefill.</li> <li>However, batching increases the chance that fast requests are delayed by slow ones.</li> <li>Optimal batch sizing balances GPU utilisation and end-to-end latency.</li> </ul> <h4>Memory Footprint</h4> <ul> <li>Caches grow with sequence length, hidden size, and number of layers.</li> <li>For high-throughput systems, careful memory management is essential, especially with long prompts or large batch sizes.</li> </ul> <h3>Decode Phase</h3> <p>After prefill, the model enters the autoregressive loop:</p> <ol> <li>Take the hidden state of the last token.</li> <li>Compute attention over cached key-value pairs.</li> <li>Predict the distribution over the vocabulary.</li> <li>Determine the next token (via greedy, top-k, nucleus sampling, or beam search).</li> <li>Append the token to the sequence.</li> <li>Update the cache with the new token’s key and value.</li> </ol> <p>This loop continues until a stop condition is met which would commonly be an end-of-sequence token or a maximum token limit.</p> <h3>Output Control Techniques</h3> <h4>Greedy Sampling</h4> <p>Greedy sampling basically chooses the next token based on the highest probability it can achieve at every step.</p> <h4>Beam Search</h4> <p>Unlike greedy sampling, Beam search maintains multiple paths in parallel, expanding the most promising sequences. This helps the model to overlook more than just the next maximising token. This improves coherence at the cost of computation.</p> <h4>Temperature and Top-k/Nucleus Sampling</h4> <ul> <li> <strong>Temperature</strong> controls the randomness of sampling. This is applied as a scaling factor for the output logits before the softmax operation to control generation to be more deterministic or non-deterministic. Higher temperature values scale the distribution more and introduce a non-deterministic nature for next token selection and vice versa.</li> <li> <strong>Top-k sampling</strong> restricts choices to the k most likely tokens and randomly samples out of the top-k tokens.</li> <li> <strong>Nucleus (top-p) sampling</strong> chooses from the smallest set of tokens whose cumulative probability exceeds p.</li> </ul> <h4>Length Penalty and Max Tokens</h4> <p>To prevent overly long or short generations, users can set maximum token limits and apply length penalties to the generation score.</p> <h4>Repetition Penalty</h4> <p>Models often repeat phrases. A repetition penalty modifies the logits of already-used tokens to discourage reuse.</p> <h3>Conclusion</h3> <p>Understanding how LLMs perform inference is vital for building efficient and responsive applications. The prefill and decode phases involve different trade-offs, from latency and memory to sampling and quality. With the right strategies like caching, batching, and careful sampling, you can harness the full potential of LLMs for your application needs.</p> <h3>References</h3> <ul><li>LLM inference: <a href="https://huggingface.co/learn/llm-course/chapter1/8?fw=pt#inference-with-llms" rel="external nofollow noopener" target="_blank">https://huggingface.co/learn/llm-course/chapter1/8?fw=pt#inference-with-llms</a> </li></ul> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=36a767f98a83" width="1" height="1" alt=""></p> </body></html>