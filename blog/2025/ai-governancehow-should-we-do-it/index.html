<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h3>AI governance -How should we do it?</h3> <p>Building AI responsibly requires collaboration among researchers, developers, regulators, and end users. Embedding ethical safeguards and governance practices throughout the AI lifecycle helps foster trust in AI systems and ensures alignment with human values.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*RZGG5kydj81yo9NS"><figcaption>Photo by <a href="https://unsplash.com/@amstram?utm_source=medium&amp;utm_medium=referral" rel="external nofollow noopener" target="_blank">Scott Graham</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="external nofollow noopener" target="_blank">Unsplash</a></figcaption></figure> <h3>What is AI Governance?</h3> <p>AI governance refers to the framework of policies, processes and practices that ensure the responsible development and use of AI systems. It aims to maximise the benefits of AI while minimising potential harms and risks, such as bias, lack of transparency and ethical concerns. Effective AI governance requires collaboration between various stakeholders, including governments, tech companies, researchers, and the public.</p> <h3>AI Governance in Practice: Google’s Approach</h3> <p>In February 2025, Google published a detailed report outlining its end-to-end AI governance framework, showcasing how it builds responsible AI at scale. The report explains how Google integrates responsible practices across infrastructure, model development, and product deployment to manage AI systems at scale.</p> <h3>What is “Full-Stack” Governance?</h3> <p>Google’s framework emphasises that governance cannot be an afterthought but must be embedded throughout the AI lifecycle, involving:</p> <ul> <li>Data curation and documentation</li> <li>Model development and evaluation</li> <li>Application design and launch</li> <li>Post-deployment monitoring and iteration</li> </ul> <p>This “full-stack” perspective combines technical, organisational, and procedural elements to embed responsibility into every stage.</p> <h3>Key highlights</h3> <p>Google’s governance principles and approach are based on the <a href="https://airc.nist.gov/airmf-resources/playbook/" rel="external nofollow noopener" target="_blank">NIST AI Risk Management Framework</a> and focus on governance, mapping, measuring, and managing risks.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/706/1*rd87MK8vXutAP1QSk-ke-g.png"><figcaption>Google’s AI governance approach</figcaption></figure> <h3>Tackling pre- and post-launch reviews of AI systems</h3> <h4>Model requirements</h4> <p>Google mandates that all models meet rigorous standards for data quality, performance, and compliance:</p> <ul> <li>Training data is filtered to remove harmful or low-quality content.</li> <li>Models must adhere to defined responsible AI principles.</li> <li>Each model is accompanied by technical reports and model cards, which document performance, intended use, and limitations.</li> </ul> <h4>Application requirements</h4> <p>Before any AI-powered application goes live, it must pass several governance gates:</p> <ul> <li>Risk assessments, testing and design guidance</li> <li>Human-in-the-loop design reviews</li> <li>Adherence to launch protocols tailored for high-risk domains (e.g., health, applications developed for minors)</li> </ul> <h4>Leadership reviews</h4> <p>One of the report’s standout features is the emphasis on cross-functional leadership reviews. Executives with expertise in responsible AI are directly involved in go/no-go decisions for launches, reinforcing accountability at the highest levels.</p> <h4>Post-launch requirements</h4> <p>Governance continues <strong>after launch</strong>, with ongoing evaluations of:</p> <ul> <li>Emerging risks (such as model drifts, misuse of products)</li> <li>User feedback and real-world performance</li> <li>Gaps in mitigation strategies and policies that only appear at scale</li> </ul> <p>Post-launch reviews are treated not as optional retrospectives, but as core governance events.</p> <h4>Infrastructure used for launching applications</h4> <p>The infrastructure is continuously streamlined for AI applications, responsibility testing and progress monitoring.</p> <h3>Documentation</h3> <p>Google places significant emphasis on rigorous model documentation as part of its AI governance framework. Technical reports are routinely published for advanced models, offering in-depth detail on model design, training data, evaluation procedures, and intended use cases. These reports are complemented by model cards, which present key information in a standardised, accessible format designed for developers and for policy stakeholders.</p> <h4>Model Cards</h4> <p>External model cards and technical reports are published regularly as transparency artefacts. Below is an example of Google’s suggestion for a model card in 2019 [<a href="https://arxiv.org/pdf/1901.10002" rel="external nofollow noopener" target="_blank">paper</a>]. While the format has evolved, the core principle of transparent, standardised model documentation gives a really good picture of the model’s development and capabilities.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/677/1*vksnw0ns5mprA9yj3EGZxg.png"><figcaption>Google’s model card suggestion from 2019</figcaption></figure> <h4>Data and Model lineage</h4> <p>To support responsible scaling of AI systems, Google is also investing in infrastructure for data and model lineage tracking, which involves maintaining end-to-end visibility into the lifecycle of datasets and models. Such lineage systems are critical for debugging, compliance, and auditability, particularly in high-stakes applications where traceability is non-negotiable. By embedding lineage as a core technical capability, Google ensures that every AI system it deploys can be examined retrospectively with clarity on provenance and transformation history.</p> <h3>Conclusion</h3> <p>Overall, Google’s report presents a thorough and technically grounded AI governance framework. Its focus on documentation, post-deployment oversight, and lineage demonstrates an institutional commitment to responsible AI.</p> <p>While the specific mechanisms may vary across organisations, the underlying principles of transparency, traceability and cross-functional oversight are essential components of responsible AI development.</p> <h3>References</h3> <ul> <li>Google’s responsible AI progress report: <a href="https://ai.google/static/documents/ai-responsibility-update-published-february-2025.pdf" rel="external nofollow noopener" target="_blank">https://ai.google/static/documents/ai-responsibility-update-published-february-2025.pdf</a> </li> <li>NIST AI Risk Management Framework: <a href="https://airc.nist.gov/airmf-resources/playbook/" rel="external nofollow noopener" target="_blank">https://airc.nist.gov/airmf-resources/playbook/</a> </li> <li>“Model Cards for Model Reporting” paper: <a href="https://arxiv.org/abs/1810.03993" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/1810.03993</a> </li> </ul> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=1d24b928135c" width="1" height="1" alt=""></p> </body></html>