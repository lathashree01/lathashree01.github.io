<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h3>Simple RAG architecture design -Indexing and Inference pipelines</h3> <p>Grounding the LLM to the data you care about.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*L4GsVEZuOEireODg"><figcaption>Photo by <a href="https://unsplash.com/@kellysikkema?utm_source=medium&amp;utm_medium=referral" rel="external nofollow noopener" target="_blank">Kelly Sikkema</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="external nofollow noopener" target="_blank">Unsplash</a></figcaption></figure> <p>As our interactions with AI and conversational systems become increasingly common, we’ve all experienced moments where a chatbot drifts off-topic, discussing irrelevant information when we’re seeking answers grounded in a specific context. The key challenge is ensuring that the chatbot stays focused on the data or document we care about and generates responses based on that context. This is precisely where <strong>Retrieval-Augmented Generation (RAG)</strong> proves valuable.</p> <h3><strong>Retrieval-Augmented Generation</strong></h3> <p>RAG is a hybrid architecture that combines two key components: a <strong>retriever</strong> and a <strong>generator</strong>.</p> <p>The retriever is responsible for fetching relevant pieces of information from a large corpus or knowledge base, while the generator (typically a large language model) uses this retrieved content to craft a context-aware response. This architecture aims to ground the model’s output in facts and domain-specific data, thus improving relevance, factual accuracy, and controllability.</p> <p>Traditional LLMs, when used in isolation, rely solely on their pre-trained parameters to answer questions. This leads to a few key limitations:</p> <ul> <li>They may hallucinate facts.</li> <li>They cannot access new or proprietary information unless they are fine-tuned on it.</li> <li>They struggle to stay within a defined domain of knowledge.</li> </ul> <p>RAG addresses these issues by introducing an <strong>external source of truth</strong> during the inference process.</p> <p>A typical RAG system consists of two main stages: <strong>indexing</strong> and <strong>inference</strong>.</p> <h4>Indexing Phase</h4> <p>This phase involves preparing and storing the documents in a form suitable for retrieval at inference time. A simple indexing pipeline may look like the following:</p> <ol> <li> <strong>Document Ingestion</strong><br> Start with a collection of documents, which may be in formats such as PDFs, Word files, HTML, or plain text.</li> <li> <strong>Parsing and Preprocessing</strong><br>Use document parsers (e.g. PDF readers) to extract raw text content. This may also include removing boilerplate, normalising whitespace, or handling non-textual elements.</li> <li> <strong>Chunking</strong><br> Split the documents into smaller, manageable chunks (by paragraphs or sliding windows of N tokens). It’s important that each chunk contains enough and semantically meaningful information, so it can provide relevant context to our LLM during generation.</li> <li> <strong>Embedding Generation</strong><br>Use a sentence embedding model (e.g. Sentence-BERT, OpenAI’s text-embedding-3-small, or Cohere embeddings) to convert each chunk into a dense vector representation.</li> <li> <strong>Storage in Vector Database</strong><br>Store these embeddings in a vector database alongside metadata like document ID, source and text. This allows for efficient similarity search for retrieval at inference time.</li> </ol> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*5p6BBWcTz1qlMLp4WqoW4g.png"><figcaption>Simple document indexing pipeline for RAG systems</figcaption></figure> <h4>Inference Phase</h4> <p>When a user submits a query, the system goes through the following steps:</p> <ol> <li> <strong>Query Embedding</strong><br>The input query is converted into a dense vector using the same embedding model employed during the indexing phase. This ensures alignment in vector space between the query and document chunks.</li> <li> <strong>Context Retrieval</strong><br>The query embedding is used to perform a similarity search, typically using cosine similarity or approximate nearest neighbours, in the vector database. This retrieves the top-K most relevant document chunks based on semantic similarity.</li> <li> <strong>LLM Input Construction</strong><br>The retrieved chunks are formatted and injected into the input prompt of the language model, along with the original user query.</li> <li> <strong>LLM Generation and Output Post-processing</strong><br>The LLM generates a response grounded in the retrieved context. This output may be post-processed to enforce constraints such as user-specific policies, safety filters, or formatting requirements. The final response is then returned to the user.</li> </ol> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*iGUAbosUUqu9lkX2pYznlA.png"><figcaption>Simple RAG-based inference pipeline</figcaption></figure> <h3>Conclusion</h3> <p>RAG provides a robust framework for building context-aware, domain-specific AI systems by combining the strengths of information retrieval with large language models. By grounding responses in retrieved content, RAG systems improve factual accuracy and enable dynamic knowledge integration.</p> <p>There are numerous optimisation strategies for deploying RAG-based applications in production. I highly encourage exploring these in depth, and I plan to share some of the techniques I have used in my own projects in upcoming articles.</p> <p>Ultimately, as the demand for trustworthy and adaptable AI grows, RAG stands out as a practical and scalable solution for real-world applications.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=17677ee3ade9" width="1" height="1" alt=""></p> </body></html>