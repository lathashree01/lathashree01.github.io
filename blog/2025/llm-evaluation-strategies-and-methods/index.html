<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <figure><img alt="Human-machine interaction" src="https://cdn-images-1.medium.com/max/848/1*Wi1yq-hZJH4ElnXhSYVtjA.jpeg"><figcaption><a href="https://www.simplilearn.com/machine-learning-vs-deep-learning-major-differences-you-need-to-know-article" rel="external nofollow noopener" target="_blank">source</a></figcaption></figure> <p>In the rapidly advancing field of AI, we are witnessing exponential advancements that are reshaping technology at a remarkable pace. The emergence of ChatGPT marked a pivotal moment in AI, sparking a global conversation about language models capable of performing tasks with exceptional proficiency, far beyond what was previously imagined !!</p> <blockquote>Artificial Intelligence is not a substitute for human intelligence; it is a tool to amplify human creativity and ingenuity. — Fei Fei Li</blockquote> <p>This article explores types of use cases that you can build using LLMs and evaluation strategies for understanding LLMs’ performance.</p> <p><strong><em>A quick recap for someone wondering what an LLM is</em></strong></p> <p>LLM is an advanced version of a language model with billions of parameters, which would be trained on vast amounts of text data. LLMs excel at both <strong>generative</strong> tasks, such as writing coherent paragraphs, and <strong>predictive</strong> tasks, like filling in missing words in sentences or classifying texts.</p> <h3><strong><em>What can we build using LLMs?</em></strong></h3> <p>Applications that integrate LLMs are often referred to as <strong>LLM-powered applications</strong>, enabling capabilities such as chatbots like ChatGPT, automated content generation, and agentic solutions. When we build such an application, it is very critical to ensure it is stable and tested thoroughly.</p> <p>LLM outputs are non-deterministic, meaning the model can generate different responses to the same input. Given this behaviour, evaluation criteria should focus on understanding both its capabilities and potential risks. For example, if we are building a Question &amp; Answering System, it is essential to check whether the chatbot is giving correct answers. Some other evaluation criteria could include — Are the answers helpful? Is the tone okay? Is the model hallucinating? Is the chatbot being fair? etc.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/693/1*w0PrITi8J_7Bu6t47VwnGQ.png"></figure> <h3><strong><em>How do we evaluate our LLM apps?</em></strong></h3> <p>There are several ways to evaluate LLMs, including manual evaluation and automated methods. One approach is to assemble a set of domain experts to create a gold-standard dataset and compare the model’s output against the experts’ expectations. However, this process can be costly and may not be feasible when building solutions for certain specialised domains, such as healthcare.</p> <p>Let’s explore different approaches for gathering evaluation datasets and the strategies you can use to assess LLM performance.</p> <p><strong><em>Ways to generate Evaluation datasets:</em></strong></p> <ul> <li> <strong>Using Publicly Available Benchmark Datasets</strong> — Leverage established benchmark datasets designed for evaluating LLMs.</li> <li> <strong>Utilising Existing Business Data</strong> — Usage of internal datasets relevant to your specific domain.</li> <li> <strong>Generating Synthetic Datasets</strong> — Create artificial data using existing source documents.</li> <li> <strong>Creating Test Cases</strong> — Design targeted test cases to assess model responses based on predefined evaluation criteria.</li> </ul> <p><strong><em>By combining all the evaluation datasets, we can group the final evaluation strategy into three main categories:</em></strong></p> <ol> <li> <strong>Happy Path</strong><br>Covers expected use cases where the application functions as intended, ensuring the model performs well in typical scenarios.</li> <li> <strong>Edge Cases</strong><br>Includes unexpected user interactions that fall outside the project scope. For example, how does the model handle off-topic questions or sensitive queries?</li> <li> <strong>Adversarial Scenarios</strong><br>Focuses on attempts by malicious users to exploit the application, such as extracting harmful content or manipulating the LLM’s responses.</li> </ol> <h3><strong><em>LLM Evaluation Methods and Metrics</em></strong></h3> <p><strong><em>Group 1:</em> Requires Ground Truth Dataset — Measures Correctness</strong></p> <p>These evaluation methods are focused on assessing the correctness of the model’s outputs, typically by comparing them to a predefined “ground truth” or expected outcome.</p> <ol><li> <strong>Predictive Quality Metrics</strong><br>Predictive quality metrics are primarily used for tasks like classification, where the goal is to predict labels based on input data. Common metrics in this category include:</li></ol> <ul> <li> <strong>Accuracy</strong>: The percentage of correct predictions out of all predictions made.</li> <li> <strong>Precision, Recall, and F1-score</strong>: Precision measures the proportion of correct positive predictions out of all positive predictions, while recall calculates the proportion of correct positive predictions out of all actual positives. The F1-score is the harmonic mean of precision and recall, providing a balance between the two.</li> <li> <strong>Area Under the Receiver Operating Characteristic Curve (AUC-ROC)</strong>: This measures the model’s ability to distinguish between classes across different thresholds.</li> </ul> <p><strong>2. Generative Quality Metrics</strong><br>In generative tasks, such as text generation or machine translation, the model is expected to produce fluent and accurate outputs. Common metrics include:</p> <ul> <li> <strong>BLEU (Bilingual Evaluation Understudy)</strong>: Measures the overlap of n-grams between the generated text and the reference text. It’s widely used in machine translation.</li> <li> <strong>ROUGE (Recall-Oriented Understudy for Gisting Evaluation)</strong>: Measures the overlap of n-grams, words, or word sequences between the model-generated output and a set of reference outputs. It’s often used in tasks like summarisation.</li> </ul> <p><strong>3. Semantic Similarity</strong><br>These methods assess how semantically similar the model-generated output is to the reference output at the embedding level. The most common approach is to measure the cosine similarity between the embeddings of the generated and reference texts. A higher cosine similarity indicates that the two pieces of text are more similar in meaning.</p> <ul><li> <strong>Cosine Similarity</strong>: Measures the cosine of the angle between two vectors (representing text in an embedding space). A cosine similarity of 1 indicates identical vectors, while 0 indicates orthogonal and -1 indicates opposite (completely dissimilar) vectors.</li></ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/768/0*Flk84kQHxk22B-Wk.png"><figcaption><a href="https://kdb.ai/learning-hub/articles/methods-of-similarity/" rel="external nofollow noopener" target="_blank">Source</a></figcaption></figure> <p><strong>4. LLM-as-a-Judge</strong><br>This method involves the LLM acting as the judge or evaluator of the potential LLM’s responses, even without a strict ground truth.</p> <p>For example, the model could assess its output based on predefined guidelines or criteria, such as coherence, relevance, and alignment with the expected output.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*hgK4CjylQhWOU3I9vZ-HOg.png"><figcaption><a href="https://www.evidentlyai.com/" rel="external nofollow noopener" target="_blank">Source</a></figcaption></figure> <blockquote>Sometimes a bigger and better LLM is used as a judge to evaluate the candidate LLM’s outputs.</blockquote> <p><strong><em>Group 2:</em> Does Not Require Ground Truth — Measures Relevance</strong></p> <p>These methods evaluate how relevant, useful, or meaningful the model’s output is, without requiring an exact “correct” answer to compare against.</p> <ol> <li> <strong>Sentiment Analysis</strong><br>Sentiment analysis evaluates the emotional tone or sentiment behind a piece of text. LLMs can be evaluated by how accurately they identify sentiments such as positive, negative, or neutral within generated content. While this doesn’t require ground truth in the traditional sense, it relies on established sentiment categories to determine relevance and accuracy.</li> <li> <strong>Regular Expressions</strong><br>Regular expressions can be used to evaluate specific patterns or structures within the generated text, such as verifying the presence of phone numbers, email addresses, or dates in a response. This type of evaluation does not depend on ground truth, but rather on whether the output meets certain predefined patterns or formats.</li> <li> <strong>LLM-as-a-Judge (Relevance Evaluation)</strong><br>Here, the LLM evaluates its response for relevance, usefulness, and alignment with the task. For instance, the model could assess whether its answer is on-topic and provides valuable information. This approach doesn’t require a ground truth but relies on predefined relevance criteria.</li> <li> <strong>Functional Testing</strong><br>Functional testing ensures that LLM-generated outputs are executable and usable in real-world contexts. Examples include:</li> </ol> <ul> <li> <strong>Executing Code</strong>: Running generated code (e.g., Python) to verify it works as intended.</li> <li> <strong>SQL Query Execution</strong>: Testing generated SQL queries by executing them against a database to ensure they return the expected results.</li> <li> <strong>APIs and Workflows</strong>: Verifying that generated API calls or workflows function as intended in a practical application.</li> </ul> <p>The focus is on the relevance and operability of the model’s outputs.</p> <p>In conclusion, evaluating LLM applications requires careful examination by combining methods that assess both the correctness and relevance of LLM’s outputs. By using techniques such as ground-truth-based metrics, functional testing, and self-assessment by another LLM, we can gain a comprehensive understanding of the model’s performance. Thorough evaluation is crucial to ensure that LLMs not only generate accurate outputs but also remain useful, relevant, and reliable in real-world applications.</p> <h3><em>References:</em></h3> <ol> <li>LLM evaluation course by Evidently AI <a href="https://www.evidentlyai.com/llm-evaluations-course" rel="external nofollow noopener" target="_blank">https://www.evidentlyai.com/llm-evaluations-course</a> </li> <li>What are LLMs? by IBM <a href="https://www.ibm.com/think/topics/large-language-models" rel="external nofollow noopener" target="_blank">https://www.ibm.com/think/topics/large-language-models</a> </li> </ol> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=1704771ed7e8" width="1" height="1" alt=""></p> </body></html>