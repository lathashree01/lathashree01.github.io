<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <blockquote>Exploring the unseen costs of building and maintaining ML systems — and how to mitigate them.</blockquote> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*F0MzBt-GHz7ZIAkK"><figcaption><a href="https://unsplash.com/@karishea?utm_source=medium&amp;utm_medium=referral" rel="external nofollow noopener" target="_blank">Unsplash</a></figcaption></figure> <p>Machine learning has revolutionised how we solve complex problems — from personalised recommendations to autonomous driving and many more. Yet, while rapid prototyping and deployment may seem like a quick win, they often come with a hidden price tag: <strong><em>Technical Debt</em></strong>. Drawing inspiration from the seminal paper “Hidden Technical Debt in Machine Learning Systems” by Sculley et al., this article delves into the unique challenges of ML technical debt along with actionable strategies for mitigating long-term maintenance costs.</p> <h3><em>What is Technical Debt in ML?</em></h3> <p>Let us first understand what we mean by technical debt in machine learning systems.</p> <p>In the software development field, <strong>technical debt</strong> (also known as <strong>design debt </strong>or <strong>code debt</strong>) refers to the implied cost of additional work in the future resulting from choosing an expedient solution over a more robust one. [<a href="https://en.wikipedia.org/wiki/Technical_debt" rel="external nofollow noopener" target="_blank">Wikipedia</a>]</p> <p>In ML systems, however, the same problem is compounded by various problems such as:</p> <ul> <li> <strong>Entanglement:</strong> Small changes in input data or features can unexpectedly ripple through the system.</li> <li> <strong>Data Dependencies:</strong> Unstable or redundant data features add complexity and risk.</li> <li> <strong>Configuration Complexity:</strong> An ever-growing number of configuration settings can lead to brittle systems.</li> </ul> <p>This debt may be difficult to detect because it exists at the system level rather than the code level. Only a small fraction of real-world ML systems are composed of ML code, as shown by the small black box in the middle. The required surrounding infrastructure is vast and complex. Hence, understanding these issues is critical as they not only affect model performance but also hinder the long-term maintainability and scalability of the entire ML system.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*P8kJgYx2yeqqgvE7pTG5QA.png"><figcaption>Real-world ML system components (source: 1)</figcaption></figure> <h3>Understanding the main causes of ML Tech Debt</h3> <h3>1. Entanglement and the CACE Principle</h3> <p>The “Changing Anything Changes Everything” (CACE) principle reflects how interconnected components in ML systems can be. A minor tweak in one part of the pipeline may trigger unexpected changes elsewhere. For example, changing the input data distribution of some features can cause a change in the importance of other features used in ML systems. This could also apply to many other hyperparameters used in the model training process.</p> <p><strong>Actionable Strategies to mitigate this:</strong></p> <ul> <li> <strong>Isolate Components:</strong> Design your model components to be independent, if possible.</li> <li> <strong>Monitor Input Distributions:</strong> Implement continuous monitoring and slice-based metrics to detect when the model’s prediction behaviour changes; this is a good indicator that something in the input has changed (unless the model itself has changed).</li> </ul> <h3>2. Undeclared Consumers</h3> <p>ML model predictions can often be left accessible to other systems. Having no visibility of who consumes the output of the model is risky because changes in the ML model affect the downstream processes that are consuming it. This can also be called “Visibility debt”.</p> <p><strong>Actionable Strategies to mitigate this:</strong></p> <ul> <li> <strong>Document Consumers:</strong> Maintain a registry of all systems that consume your model outputs.</li> <li> <strong>Access Controls:</strong> Implement strict access policies and service-level agreements (SLAs) to manage data usage.</li> </ul> <h3>3. Data Dependencies</h3> <p>Data is the lifeblood of any ML system. However, unstable or underutilised data signals can become significant liabilities more than the code dependencies.</p> <p><strong>Actionable Strategies to mitigate this:</strong></p> <ul> <li> <strong>Versioning:</strong> Freeze versions of critical data features to prevent sudden, detrimental changes.</li> <li> <strong>Feature Audits to remove underutilised dependencies:</strong> Conduct regular leave-one-feature-out evaluations to identify and retire redundant or low-value features.</li> <li> <strong>Automated Dependency Tracking:</strong> Use tooling to annotate and validate the relationships between features.</li> </ul> <h3>4. Configuration Debt and Pipeline Jungles</h3> <p>Over time, configurations and data pipelines can become so chucky. Having misconfigured pipelines is expensive in terms of wasted effort money and unnecessary delays. We should always think that a highly configurable system is more mature because there is less room for errors. Let us think about the principles of good configuration systems.</p> <ul> <li>Easy to specify a configuration and modify from the previous version</li> <li>It should be hard to make manual errors in the configuration. It's absolutely necessary to validate the configuration values!</li> <li>Detect unused and redundant settings left from previous iterations</li> <li>It should be part of the code repository and versioned like other code.</li> </ul> <h3>5. Changes in the External World</h3> <p>Changes in the external world, such as shifts in data sources, regulatory requirements, or technological landscapes, can cause your models to not work as well as they used to. This can affect how reliable and accurate your predictions are.</p> <p><strong>Actionable Strategies to mitigate this:</strong></p> <h4><strong>Model Monitoring:</strong></h4> <ul> <li> <strong>Why it’s important: </strong>Monitoring is like keeping a close eye on your model after it’s been put to work. It helps you see if it’s still performing well over time.</li> <li> <strong>How to do it:</strong> Set up dashboards to track key performance indicators (KPIs), data drift, and system anomalies. Think of KPIs as vital signs for your model.</li> <li> <strong>Data Drift:</strong> Data drift is when the data your model is seeing in the real world starts to look different from the data it was trained on.</li> <li> <strong>Why retrain: </strong>Monitoring helps you decide when to retrain your model with new data. Models can become “stale” if they’re not updated, which can lead to less accurate results.</li> </ul> <h4><strong>End-to-End Testing:</strong></h4> <ul> <li> <strong>What it is:</strong> This involves testing the entire data flow, from where the data comes in (ingestion) to the final prediction.</li> <li> <strong>Why it’s important:</strong> Before putting a new model into production, you want to make sure it’s better than the one you’re already using.</li> <li> <strong>How to do it:</strong> Develop comprehensive tests that simulate the entire data flow.</li> </ul> <p>There are lots of other causes for ML tech debt that are mentioned in the paper; I chose to explain some of the causes that strongly resonate with my experience. Please look into the reference section to learn more about other cases.</p> <h3>Practical Guidelines for Practitioners</h3> <ul> <li>Practice data validation and check for data leakage between your training and validation</li> <li>Version critical data features to track changes.</li> <li>Isolate model components using a modular design.</li> <li>Document your customers, specifying who your model’s direct consumers are.</li> <li>Audit code &amp; config regularly for security and dependency updates.</li> <li>Maintain a model registry like MLflow or Weights &amp; Biases.</li> <li>Ensure reproducibility of model deployments.</li> <li>Use gradual rollouts: canary, A/B testing. I wrote an article on model deployments, check it out: <a href="https://lathashreeh.medium.com/deploying-machine-learning-models-in-production-ea38bad3fe82" rel="external nofollow noopener" target="_blank">Deploying Machine Learning Models in Production</a> </li> <li>Monitor and test ML systems, and make sure to have alerting systems.</li> <li>Production model degrades, decide on a retraining period and update the model regularly.</li> <li>Last but not least, use diagrams to visualise your dataflow and the entire system design. Don't underestimate the need for good documentation :p</li> </ul> <h3>Conclusion and Future Outlook</h3> <p>Managing technical debt in ML systems is an ongoing challenge. Recognising the hidden costs early and implementing strategies to mitigate them can save considerable time and resources in the long run. As ML systems continue to grow in complexity, the development of standardised tools and frameworks will be crucial. I encourage ML practitioners to take a proactive stance — audit your systems, simplify where possible, and share your learnings with the community.</p> <p><em>What steps have you taken to manage technical debt in your ML projects? <br>Share your experiences in the comments below, and let’s build a more maintainable future for machine learning together.</em></p> <h3>References:</h3> <ol><li>D. Sculley et. al., Hidden Technical Debt in Machine Learning Systems, <a href="https://proceedings.neurips.cc/paper_files/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf" rel="external nofollow noopener" target="_blank">https://proceedings.neurips.cc/paper_files/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf</a> </li></ol> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=27fa1b13040c" width="1" height="1" alt=""></p> </body></html>