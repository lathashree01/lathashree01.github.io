<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h3>Understanding Types of training for LLMs</h3> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*OJc3QfmEJM3aMBSt"><figcaption>Photo by <a href="https://unsplash.com/@tjerwin?utm_source=medium&amp;utm_medium=referral" rel="external nofollow noopener" target="_blank">Trent Erwin</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="external nofollow noopener" target="_blank">Unsplash</a></figcaption></figure> <h3>Pretraining, Fine-tuning, and Instruction Tuning Explained</h3> <p>The rise of Large Language Models (LLMs) like GPT, Gemini, and LLaMA has revolutionised natural language processing by enabling models to perform diverse tasks with minimal task-specific supervision. Central to their effectiveness is the <strong>multi-stage training process</strong> these models undergo, training phases such as <strong>pretraining</strong>, <strong>fine-tuning</strong>, and increasingly, <strong>instruction tuning </strong>for certain use cases where a user instruction needs to be followed.</p> <p>Understanding these stages is essential for aspiring ML engineers and organisations leveraging foundation models. Here’s a short breakdown of what each training phase entails, why it matters, and how they differ.</p> <h3>Pretraining: Learning the Foundations</h3> <p><strong>Objective:</strong> Equip the model with a general understanding of language, grammar, reasoning patterns, and world knowledge.</p> <p><strong>Approach:</strong><br> Pretraining is carried out on vast amounts of <strong>unlabelled text</strong> using <strong>self-supervised learning</strong>. In the case of GPT-style models (decoder-only), this means <strong>causal language modelling</strong> — predicting the next token given the previous ones.</p> <p><strong>Data Sources:</strong></p> <ul> <li>Books, articles, websites (e.g., Common Crawl)</li> <li>Code (e.g., GitHub)</li> <li>Multilingual corpora</li> </ul> <p><strong>Outcome:</strong><br> The model becomes a <strong>foundation model</strong> — a general-purpose LLM capable of understanding and generating natural language.</p> <blockquote>Example:<em> GPT-3 is pretrained on 300+ billion tokens from a wide range of internet sources, without any explicit task-specific labels.</em> </blockquote> <h3>Post Training</h3> <h3>Fine-Tuning: Specialising for Specific Tasks</h3> <p><strong>Objective:</strong> Adapt the pretrained model to perform a <strong>specific downstream task</strong> such as sentiment analysis, named entity recognition, summarisation, or domain-specific QA.</p> <p><strong>Approach:</strong><br>Fine-tuning uses <strong>supervised learning</strong> on <strong>labelled datasets</strong>, where the model is trained to map from inputs to expected outputs.</p> <ul> <li>Often involves a smaller, task-specific dataset.</li> <li>Can update all model parameters (full fine-tuning) or just a subset (e.g. adapters, LoRA, or QLoRA).</li> </ul> <p><strong>Examples of Tasks:</strong></p> <ul> <li>Text classification</li> <li>Medical or legal domain QA</li> <li>Multimodal extensions (e.g., VQA)</li> </ul> <p><strong>Why it matters:</strong><br> Fine-tuning enables customisation and improves performance in specialised contexts, making the model practical for real-world applications.</p> <blockquote>Example:<em> Fine-tuning GPT on financial documents to improve reasoning over balance sheets and contracts.</em> </blockquote> <h3>Supervised Fine-tuning or Instruction Tuning: Making Models Follow Directions</h3> <p><strong>Objective:</strong> Teach the model to <strong>follow instructions</strong> and perform multiple tasks based on natural language prompts.</p> <p><strong>Approach:</strong><br>Instruction tuning is a form of <strong>supervised fine-tuning</strong>, but instead of training on isolated tasks, the model is trained on a diverse set of <strong>instruction–response pairs</strong>.</p> <ul> <li>Prompt: “Translate this sentence into French.”</li> <li>Response: “Voici la phrase traduite en français.”</li> </ul> <p>Instruction tuning is typically done after pretraining (and sometimes after fine-tuning), enabling models to generalise to unseen instructions.</p> <p><strong>Why it matters:</strong><br> This step dramatically improves usability by aligning the model’s behaviour with human expectations, making it more helpful, reliable, and controllable.</p> <blockquote>Example:<em> InstructGPT outperforms GPT-3 on many NLP tasks despite being smaller, thanks to instruction tuning on prompt–response examples.</em> </blockquote> <h3>Preference Tuning (PreFT)</h3> <p>A post-training technique that aligns the LLM to human preferences. Initially collect human preferences and use that data to align LLMs</p> <h3>Conclusion</h3> <p>The journey from raw text to task-aware intelligence involves strategically combining <strong>pretraining</strong>, <strong>fine-tuning</strong>, and <strong>instruction tuning</strong>. Each phase builds upon the previous one, starting from foundational knowledge, to specialised skills, and ultimately to usable, aligned behaviour.</p> <p>As models continue to scale and evolve, mastering these training paradigms is crucial for deploying LLMs safely and effectively in real-world systems.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c728ec8b205b" width="1" height="1" alt=""></p> </body></html>